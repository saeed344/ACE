{"metadata":{"colab":{"provenance":[{"file_id":"1cHuo59Bu0nOd_X98BpwV08W4q_5GfNqJ","timestamp":1686146429016}],"authorship_tag":"ABX9TyN1lQuqcczGw/4KdewMoUxU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"2iWxHp8CJVI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip  install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcqJHjkpJcKx","executionInfo":{"status":"ok","timestamp":1686146302972,"user_tz":-300,"elapsed":15245,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"a90e3574-a5eb-4171-ff55-bf0bc4bc5144","execution":{"iopub.status.busy":"2024-07-10T12:36:33.257586Z","iopub.execute_input":"2024-07-10T12:36:33.258012Z","iopub.status.idle":"2024-07-10T12:36:45.379883Z","shell.execute_reply.started":"2024-07-10T12:36:33.257983Z","shell.execute_reply":"2024-07-10T12:36:45.378581Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\ndef read_peptide_sequences(file):\n    if not os.path.exists(file):\n        print(f'Error: file {file} does not exist.')\n        sys.exit(1)\n    \n    with open(file) as f:\n        records = f.read()\n    \n    if '>' not in records:\n        print(f'Error: the input file {file} seems not in FASTA format!')\n        sys.exit(1)\n    \n    records = records.split('>')[1:]\n    peptide_sequences = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        header, sequence = array[0], ''.join(array[1:]).upper()\n        peptide_sequences.append(sequence)\n    \n    return peptide_sequences\n\ndef extract_features(sequence, tokenizer, model):\n    encoded_input = tokenizer.encode_plus(\n        sequence,\n        add_special_tokens=True,\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n\n    with torch.no_grad():\n        outputs = model(**encoded_input)\n\n    hidden_states = outputs.last_hidden_state\n    pooled_output = torch.mean(hidden_states, dim=1).squeeze()\n    features = pooled_output.numpy()\n\n    return features\n\ndef main():\n    file_path = '/kaggle/input/ace-dataset/ACE_full_dataset.txt'\n    output_csv = '/kaggle/working//bert_features_ACE.csv'\n\n    # Load pre-trained BERT model and tokenizer\n    model_name = 'bert-base-uncased'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model = BertModel.from_pretrained(model_name)\n\n    # Read peptide sequences\n    peptide_sequences = read_peptide_sequences(file_path)\n\n    # Extract BERT features for each sequence\n    features_list = []\n    for seq in peptide_sequences:\n        features = extract_features(seq, tokenizer, model)\n        features_list.append(features)\n\n    # Save features to CSV\n    features_df = pd.DataFrame(features_list)\n    features_df.to_csv(output_csv, index=False)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T12:37:12.853297Z","iopub.execute_input":"2024-07-10T12:37:12.854104Z","iopub.status.idle":"2024-07-10T12:51:12.857647Z","shell.execute_reply.started":"2024-07-10T12:37:12.854065Z","shell.execute_reply":"2024-07-10T12:51:12.856384Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\nimport pandas as pd\nimport numpy as np\nimport re, os, sys\nfrom itertools import product\n\ndef read_nucleotide_sequences(file):\n    if os.path.exists(file) == False:\n        print('Error: file %s does not exist.' % file)\n        sys.exit(1)\n    with open(file) as f:\n        records = f.read()\n    if re.search('>', records) == None:\n        print('Error: the input file %s seems not in FASTA format!' % file)\n        sys.exit(1)\n    records = records.split('>')[1:]\n    fasta_sequences = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        header, sequence = array[0].split()[0], re.sub('[^ACGTU-]', '-', ''.join(array[1:]).upper())\n        header_array = header.split('|')\n        name = header_array[0]\n        label = header_array[1] if len(header_array) >= 2 else '0'\n        label_train = header_array[2] if len(header_array) >= 3 else 'training'\n        sequence = re.sub('U', 'T', sequence)\n        fasta_sequences.append(sequence)\n    return fasta_sequences\n\n#!/usr/bin/env python\n#_*_coding:utf-8_*_\n\nimport re\n\ndef check_fasta_with_equal_length(fastas):\n    status = True\n    lenList = set()\n    for i in fastas:\n        lenList.add(len(i[1]))\n    if len(lenList) == 1:\n        return True\n    else:\n        return False\n\ndef get_min_sequence_length(fastas):\n    minLen = 10000\n    for i in fastas:\n        if minLen > len(i[1]):\n            minLen = len(i[1])\n    return minLen\n\ndef get_min_sequence_length_1(fastas):\n    minLen = 10000\n    for i in fastas:\n        if minLen > len(re.sub('-', '', i[1])):\n            minLen = len(re.sub('-', '', i[1]))\n    return minLen\ndef readFasta(file):\n    if os.path.exists(file) == False:\n        print('Error: \"' + file + '\" does not exist.')\n        sys.exit(1)\n\n    with open(file) as f:\n        records = f.read()\n\n    if re.search('>', records) == None:\n        print('The input file seems not in fasta format.')\n        sys.exit(1)\n\n    records = records.split('>')[1:]\n    myFasta = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n        myFasta.append([name, sequence])\n    return myFasta\n\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n\ndef extract_features(sequence):\n    # Tokenize the input sequence\n    encoded_input = tokenizer.encode_plus(\n        sequence,\n        add_special_tokens=True,\n        padding='max_length',\n        truncation=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n\n    # Pass the tokenized input through the BERT model\n    with torch.no_grad():\n        outputs = model(**encoded_input)\n\n    # Extract the hidden states from BERT's output\n    hidden_states = outputs.last_hidden_state\n\n    # Perform pooling to obtain a fixed-size feature vector\n    pooled_output = torch.mean(hidden_states, dim=1).squeeze()\n\n    # Convert the feature vector to a numpy array\n    features = pooled_output.numpy()\n\n    return features\n    \n\n# Example usage\ndna_sequences = ['ATCGATCGATCG', 'CGATCGATCGATCG']\n\nfastas = read_nucleotide_sequences('/content/drive/MyDrive/Smile_feature/AMPylation_data.fasta')\nfeatures = extract_features(fastas)\ndata_csv=pd.DataFrame(features)\ndata_csv.to_csv('/content/drive/MyDrive/Smile_feature/FastText_feature_dna.csv')\n\nprint(features)\n","metadata":{"id":"7FlchyaaJb6n"},"execution_count":null,"outputs":[]}]}