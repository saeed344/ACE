{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8809486,"sourceType":"datasetVersion","datasetId":5298654}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-04T20:12:15.974236Z","iopub.execute_input":"2024-07-04T20:12:15.974662Z","iopub.status.idle":"2024-07-04T20:12:17.499057Z","shell.execute_reply.started":"2024-07-04T20:12:15.974625Z","shell.execute_reply":"2024-07-04T20:12:17.497618Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ace-dataset/ACE_dataset.csv\n/kaggle/input/ace-dataset/ACE_dataset.fasta\n/kaggle/input/ace-dataset/features/ACE_ASDC.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_1_features.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_2_features.csv\n/kaggle/input/ace-dataset/features/esmv1_feat_ACE.csv\n/kaggle/input/ace-dataset/features/ACE_embeddings_prot_t5_xl_bfd.csv\n/kaggle/input/ace-dataset/features/esm2_t6_8M_feat_ACE.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_3_features.csv\n/kaggle/input/ace-dataset/features/opf_10bit_features.csv\n/kaggle/input/ace-dataset/features/ACE_AAC.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-500.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-300.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-50.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-400.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-450.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-350.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-250.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-200.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-100.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-150.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:17.500706Z","iopub.execute_input":"2024-07-04T20:12:17.501211Z","iopub.status.idle":"2024-07-04T20:12:17.507495Z","shell.execute_reply.started":"2024-07-04T20:12:17.501174Z","shell.execute_reply":"2024-07-04T20:12:17.505972Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:37.298763Z","iopub.execute_input":"2024-07-04T20:12:37.299175Z","iopub.status.idle":"2024-07-04T20:12:37.305310Z","shell.execute_reply.started":"2024-07-04T20:12:37.299140Z","shell.execute_reply":"2024-07-04T20:12:37.303783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"rnnamp_model = pd.read_csv('/kaggle/input/ace-dataset/ACE_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:42.859265Z","iopub.execute_input":"2024-07-04T20:12:42.860161Z","iopub.status.idle":"2024-07-04T20:12:42.884958Z","shell.execute_reply.started":"2024-07-04T20:12:42.860118Z","shell.execute_reply":"2024-07-04T20:12:42.883615Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X=rnnamp_model['text']\ny=np.array(rnnamp_model['label'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:47.071189Z","iopub.execute_input":"2024-07-04T20:12:47.071622Z","iopub.status.idle":"2024-07-04T20:12:47.083336Z","shell.execute_reply.started":"2024-07-04T20:12:47.071588Z","shell.execute_reply":"2024-07-04T20:12:47.081800Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,models\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import train_test_split\nfrom keras.saving import register_keras_serializable\nfrom tensorflow.keras import regularizers\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:45:15.907645Z","iopub.execute_input":"2024-07-04T20:45:15.908124Z","iopub.status.idle":"2024-07-04T20:45:15.916512Z","shell.execute_reply.started":"2024-07-04T20:45:15.908087Z","shell.execute_reply":"2024-07-04T20:45:15.915059Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:13:42.105244Z","iopub.execute_input":"2024-07-04T20:13:42.106193Z","iopub.status.idle":"2024-07-04T20:13:42.119617Z","shell.execute_reply.started":"2024-07-04T20:13:42.106151Z","shell.execute_reply":"2024-07-04T20:13:42.118137Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"'''\n# Example data\ntexts = X_train\n\n# Tokenize the texts\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts)\n\n# Convert text to sequences of integers\nsequences = tokenizer.texts_to_sequences(texts)\ntest_seq=tokenizer.texts_to_sequences(X_test)\n# Pad sequences to a fixed length\nmaxlen = 100  # Adjust as needed based on your data\npadded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\ntest=pad_sequences(test_seq, maxlen=maxlen, padding='post', truncating='post')\n# Convert to NumPy array\nX_train = tf.constant(padded_sequences)\nX_test = tf.constant(test)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(X_train)\nprint(X_train.shape)\nX_test=np.array(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:13:45.101060Z","iopub.execute_input":"2024-07-04T20:13:45.102003Z","iopub.status.idle":"2024-07-04T20:13:45.108758Z","shell.execute_reply.started":"2024-07-04T20:13:45.101962Z","shell.execute_reply":"2024-07-04T20:13:45.107303Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(816,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenizer(text, max_len):\n    dic = {'A': 1, 'G': 2, 'V': 3, 'I': 4, 'L': 5, 'F': 6, 'P': 7, 'Y': 8,\n           'M': 9, 'T': 10, 'S': 11, 'H': 12, 'N': 13, 'Q': 14, 'W': 15,\n           'K': 16, 'R': 17, 'D': 18, 'E': 19, 'C': 20}\n    \n    onehot = []\n    t = []\n    \n    for i in range(len(text)):\n        row = []\n        l = []\n        chars = text[i].split()  # split by default splits by any whitespace\n        for j in range(max_len):\n            if j < len(chars):\n                if chars[j] in dic:\n                    row.append(dic[chars[j]])\n                    r = np.zeros(20)\n                    r[dic[chars[j]] - 1] = 1\n                else:\n                    # Handle unknown characters\n                    row.append(0)  # or another default value\n                    r = np.ones(20) * -1\n            else:\n                r = np.ones(20) * -1\n                row.append(0)\n            l.append(r)\n        l = np.array(l)\n        onehot.append(l)\n        t.append(row)\n    \n    onehot = np.array(onehot)\n    t = np.array(t)\n    \n    return t, onehot\nmax_len=30\n\nX_train,onehot_train=tokenizer(X_train,max_len)\nX_test,onehot_test=tokenizer(X_test,max_len)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:43:30.081365Z","iopub.execute_input":"2024-06-28T09:43:30.081795Z","iopub.status.idle":"2024-06-28T09:43:30.347525Z","shell.execute_reply.started":"2024-06-28T09:43:30.081761Z","shell.execute_reply":"2024-06-28T09:43:30.346080Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"X_train:\\n\", X_train)\nprint(\"onehot_train:\\n\", onehot_train)\nprint(\"X_test:\\n\", X_test)\nprint(\"onehot_test:\\n\", onehot_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:34:54.035071Z","iopub.execute_input":"2024-06-28T09:34:54.035501Z","iopub.status.idle":"2024-06-28T09:34:54.048326Z","shell.execute_reply.started":"2024-06-28T09:34:54.035469Z","shell.execute_reply":"2024-06-28T09:34:54.047110Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"X_train:\n [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nonehot_train:\n [[[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n ...\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]]\nX_test:\n [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nonehot_test:\n [[[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n ...\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"X_train:\\n\", X_train.shape)\nprint(\"onehot_train:\\n\", onehot_train.shape)\nprint(\"X_test:\\n\", X_test.shape)\nprint(\"onehot_test:\\n\", onehot_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:35:14.110563Z","iopub.execute_input":"2024-06-28T09:35:14.111013Z","iopub.status.idle":"2024-06-28T09:35:14.117143Z","shell.execute_reply.started":"2024-06-28T09:35:14.110977Z","shell.execute_reply":"2024-06-28T09:35:14.115895Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"X_train:\n (816, 30)\nonehot_train:\n (816, 30, 20)\nX_test:\n (204, 30)\nonehot_test:\n (204, 30, 20)\n","output_type":"stream"}]},{"cell_type":"code","source":"path = \"/kaggle/input/ace-dataset/auto_enco_feat/\"\ndata_ = pd.read_csv(path + 'AEDN-100.csv')\n\ndata_np = np.array(data_)\ndata = data_np[:, 1:]\n# data = data_.iloc[:, :-1].values\n# labels = data_.iloc[:, -1].values\n\nlabel1=np.ones((394,1))#Value can be changed\nlabel2=np.zeros((626,1))\nlabels=np.append(label1,label2)\n\nX_train, X_test, y_train, y_test= train_test_split(data,labels, test_size=0.2, random_state=0)\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:16:25.424757Z","iopub.execute_input":"2024-07-04T20:16:25.425159Z","iopub.status.idle":"2024-07-04T20:16:25.465699Z","shell.execute_reply.started":"2024-07-04T20:16:25.425124Z","shell.execute_reply":"2024-07-04T20:16:25.464517Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(816, 100)\n","output_type":"stream"}]},{"cell_type":"code","source":"def positional_encoding(positions, d):\n\n    # initialize a matrix angle_rads of all the angles\n    pos=np.arange(positions)[:, np.newaxis] #Column vector containing the position span [0,1,..., positions]\n    k= np.arange(d)[np.newaxis, :]  #Row vector containing the dimension span [[0, 1, ..., d-1]]\n    i = k//2\n    angle_rads = pos/(10000**(2*i/d)) #Matrix of angles indexed by (pos,i)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    #adds batch axis\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:16:38.722897Z","iopub.execute_input":"2024-07-04T20:16:38.723356Z","iopub.status.idle":"2024-07-04T20:16:38.732367Z","shell.execute_reply.started":"2024-07-04T20:16:38.723304Z","shell.execute_reply":"2024-07-04T20:16:38.731051Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"@register_keras_serializable()\nclass TransformerModel(keras.Model):\n    def __init__(self, input_vocab_size, d_model, num_heads, ff_dim, rate=0.1, maxlen=50):\n        super(TransformerModel, self).__init__()\n\n        self.embedding = layers.Embedding(input_vocab_size, d_model)\n        self.PE = positional_encoding(maxlen, d_model)\n        self.transformer_block = TransformerBlock_Encode(d_model, num_heads, ff_dim, rate)\n        self.transformer_block2 = TransformerBlock_decode(d_model, num_heads, ff_dim, rate)\n        self.flatten = layers.Flatten()\n        self.fc1 = layers.Dense(512, activation=\"relu\")\n        self.fc3 = layers.Dense(256, activation=\"relu\")\n        self.fc2 = layers.Dense(1, activation=\"sigmoid\")\n\n    def call(self, inputs, training):\n        x = self.embedding(inputs)\n        #x = x+self.PE\n        y = self.transformer_block(x)\n        x = self.transformer_block2(x,y,y)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc3(x)\n        return self.fc2(x)\n\nclass TransformerBlock_decode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_decode, self).__init__()\n\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n        self.dropout3 = layers.Dropout(rate)\n\n    def call(self, inputs, q, k, training=None):\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n        attn_output1 = self.att1(q, k, out1)\n        out2 = self.layernorm2(out1 + self.dropout2(attn_output1, training=training))\n\n        ffn_output = self.ffn(out2)\n        return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n\n\nclass TransformerBlock_Encode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_Encode, self).__init__()\n        self.con= layers.Conv1D(256,5,padding='same')\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training=None):\n        inputs = self.con(inputs)\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n        \n        ffn_output = self.ffn(out1)\n        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n    \n# class TransformerBlock_decode(layers.Layer):\n#     def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n#         super(TransformerBlock_decode, self).__init__()\n\n#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n#         self.dropout3 = layers.Dropout(rate)\n\n#     def call(self, inputs, q, k, training=None):\n#         attn_output = self.att(inputs, inputs, inputs)\n#         out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n#         attn_output1 = self.att1(q, k, out1)\n#         out2 = self.layernorm2(out1 + self.dropout2(attn_output1, training=training))\n\n#         ffn_output = self.ffn(out2)\n#         return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n\n\n# class TransformerBlock_Encode(layers.Layer):\n#     def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n#         super(TransformerBlock_Encode, self).__init__()\n#         self.con= layers.Conv1D(256,5,padding='same')\n#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs, training=None):\n#         inputs = self.con(inputs)\n#         attn_output = self.att(inputs, inputs, inputs)\n#         out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n        \n#         ffn_output = self.ffn(out1)\n#         return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n\n\n# Define your data loading and preprocessing here\n# Example: X_train, y_train = load_data_and_preprocess()\n\n# Define the model\ninput_vocab_size = 1024  # Replace with the actual vocabulary size\nd_model = 256\nnum_heads = 8\nff_dim = 256\n\nmodel = TransformerModel(input_vocab_size, d_model, num_heads, ff_dim)\ninitial_learning_rate = 0.0001\n\noptimizer = Adam(learning_rate=initial_learning_rate, clipvalue=0.5)\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n\nfilepath = \"/kaggle/working/\"\nlr_scheduler = LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\ncallback = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n            tf.keras.callbacks.ModelCheckpoint(filepath='.weights.h5', monitor='val_accuracy', save_best_only=True, mode='auto', save_weights_only=True),\n            lr_scheduler]\n# Train the model\nhistory = model.fit(X_train,y_train,epochs = 100,batch_size=32,validation_split=0.1,callbacks=[callback],verbose=1,shuffle= True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-04T20:45:22.907088Z","iopub.execute_input":"2024-07-04T20:45:22.907563Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m 1/23\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:18\u001b[0m 14s/step - accuracy: 0.5625 - loss: 5.9447","output_type":"stream"}]},{"cell_type":"code","source":"model.load_weights('/kaggle/input/amap/tensorflow2/model/1/ACE.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1=model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection, metrics\ny_pred=model.predict(X_train)\nfpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred, pos_label=1)\nauc=metrics.auc(fpr, tpr)\n\ny_pred[y_pred>0.5]=1\ny_pred[y_pred<0.5]=0\n\ncv_preds = y_pred\nprint('combined train datasets')\nname='deep learning'\nprint(\"%s: Accuracy %0.2f%%\" % (name, 100*metrics.accuracy_score(y_train, cv_preds)))\nprint(\"%s: Precision-Recall %0.2f%%\" % (name, 100*metrics.average_precision_score(y_train, cv_preds)))\nprint(\"%s: Matthews Coefficient %0.2f%%\" % (name, 100*metrics.matthews_corrcoef(y_train, cv_preds)))\nprint(\"%s: Cohen Kappa Score %0.2f%%\" % (name, 100*metrics.cohen_kappa_score(y_train, cv_preds)))\nprint(\"%s: F1-Score %0.2f%%\" % (name, 100*metrics.f1_score(y_train, cv_preds)))\nprint(\"%s: AUC Score %0.2f%%\" % (name, 100*auc))\ntarget_names = ['low 0', 'high 1']\nprint(metrics.classification_report(y_train, cv_preds, target_names=target_names))\n\n# Predictions Validation Set\nprint('combined test datasets')\ny_pred2=model.predict(X_test)\nl=np.zeros(len(y_pred2))\nl=l.reshape(-1,1)\nl[y_pred2>=0.5]=1\nl[y_pred2<0.5]=0\ncv_preds2 = l\nprint(\"%s: Accuracy %0.2f%%\" % (name, 100*metrics.accuracy_score(y_test, cv_preds2)))\nprint(\"%s: Precision %0.2f%%\" % (name, 100*metrics.precision_score(y_test, cv_preds2)))\nprint(\"%s: Recall %0.2f%%\" % (name, 100*metrics.recall_score(y_test, cv_preds2)))\nprint(\"%s: Matthews Coefficient %0.2f%%\" % (name, 100*metrics.matthews_corrcoef(y_test, cv_preds2)))\nprint(\"%s: Cohen Kappa Score %0.2f%%\" % (name, 100*metrics.cohen_kappa_score(y_test, cv_preds2)))\nprint(\"%s: F1-Score %0.2f%%\" % (name, 100*metrics.f1_score(y_test, cv_preds2)))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred2, pos_label=1)\nauc=metrics.auc(fpr, tpr)\nprint(\"%s: AUC Score %0.2f%%\" % (name, 100*auc))\n\ntarget_names = ['low 0', 'high 1']\nprint(metrics.classification_report(y_test, cv_preds2, target_names=target_names))","metadata":{},"execution_count":null,"outputs":[]}]}