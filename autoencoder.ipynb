{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8820078,"sourceType":"datasetVersion","datasetId":5298654}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nprint(tf.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:27:53.447710Z","iopub.execute_input":"2024-06-29T19:27:53.448268Z","iopub.status.idle":"2024-06-29T19:27:53.455566Z","shell.execute_reply.started":"2024-06-29T19:27:53.448224Z","shell.execute_reply":"2024-06-29T19:27:53.454127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass Autoencoder(object):\n    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, transfer_function=tf.nn.softplus, optimizer=tf.keras.optimizers.Adam()):\n        self.n_input = n_input\n        self.n_hidden1 = n_hidden1\n        self.n_hidden2 = n_hidden2\n        self.n_hidden3 = n_hidden3\n        self.transfer = transfer_function\n        \n        # Define encoder weights and biases\n        self.W1 = tf.Variable(tf.random.normal([self.n_input, self.n_hidden1], dtype=tf.float32))\n        self.b1 = tf.Variable(tf.zeros([self.n_hidden1], dtype=tf.float32))\n        self.W2 = tf.Variable(tf.random.normal([self.n_hidden1, self.n_hidden2], dtype=tf.float32))\n        self.b2 = tf.Variable(tf.zeros([self.n_hidden2], dtype=tf.float32))\n        self.W3 = tf.Variable(tf.random.normal([self.n_hidden2, self.n_hidden3], dtype=tf.float32))\n        self.b3 = tf.Variable(tf.zeros([self.n_hidden3], dtype=tf.float32))\n        \n        # Define decoder weights and biases\n        self.W4 = tf.Variable(tf.random.normal([self.n_hidden3, self.n_hidden2], dtype=tf.float32))\n        self.b4 = tf.Variable(tf.zeros([self.n_hidden2], dtype=tf.float32))\n        self.W5 = tf.Variable(tf.random.normal([self.n_hidden2, self.n_hidden1], dtype=tf.float32))\n        self.b5 = tf.Variable(tf.zeros([self.n_hidden1], dtype=tf.float32))\n        self.W6 = tf.Variable(tf.random.normal([self.n_hidden1, self.n_input], dtype=tf.float32))\n        self.b6 = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n        \n        self.optimizer = optimizer\n    \n    def encoder(self, x):\n        hidden1 = self.transfer(tf.add(tf.matmul(x, self.W1), self.b1))\n        hidden2 = self.transfer(tf.add(tf.matmul(hidden1, self.W2), self.b2))\n        hidden3 = self.transfer(tf.add(tf.matmul(hidden2, self.W3), self.b3))\n        return hidden3\n    \n    def decoder(self, encoded):\n        hidden1 = self.transfer(tf.add(tf.matmul(encoded, self.W4), self.b4))\n        hidden2 = self.transfer(tf.add(tf.matmul(hidden1, self.W5), self.b5))\n        reconstruction = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, self.W6), self.b6))\n        return reconstruction\n    \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \n    def compute_loss(self, x):\n        reconstruction = self.call(x)\n        loss = tf.reduce_mean(tf.square(x - reconstruction))\n        return loss\n    \n    def partial_fit(self, X):\n        with tf.GradientTape() as tape:\n            cost = self.compute_loss(X)\n        trainable_vars = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3,\n                          self.W4, self.b4, self.W5, self.b5, self.W6, self.b6]\n        grads = tape.gradient(cost, trainable_vars)\n        self.optimizer.apply_gradients(zip(grads, trainable_vars))\n        return cost\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:28:54.095040Z","iopub.execute_input":"2024-06-29T19:28:54.095588Z","iopub.status.idle":"2024-06-29T19:28:54.210549Z","shell.execute_reply.started":"2024-06-29T19:28:54.095545Z","shell.execute_reply":"2024-06-29T19:28:54.208737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\nclass MaskingNoiseAutoencoder(object):\n    def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus, optimizer=tf.keras.optimizers.Adam(),\n                 dropout_probability=0.95):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.transfer = transfer_function\n        self.dropout_probability = dropout_probability\n\n        self.weights = self._initialize_weights()\n\n        # Model inputs\n        self.x = tf.keras.layers.Input(shape=(self.n_input,))\n        self.keep_prob = tf.keras.layers.Input(dtype=tf.float32)\n\n        # Encoder\n        self.hidden = self.transfer(tf.add(tf.matmul(tf.nn.dropout(self.x, rate=1 - self.keep_prob), self.weights['w1']),\n                                           self.weights['b1']))\n\n        # Decoder\n        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2'])\n\n        # Define optimizer and cost function\n        self.optimizer = optimizer\n        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n\n        # Compile model\n        self.autoencoder = tf.keras.Model(inputs=[self.x, self.keep_prob], outputs=self.reconstruction)\n        self.autoencoder.compile(optimizer=self.optimizer, loss='mse')\n\n    def _initialize_weights(self):\n        all_weights = {}\n        all_weights['w1'] = tf.Variable(tf.random.normal([self.n_input, self.n_hidden], dtype=tf.float32))\n        all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32))\n        all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32))\n        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        X = np.array(X)\n        cost = self.autoencoder.train_on_batch([X, self.dropout_probability], X)\n        return cost\n\n    def calc_total_cost(self, X):\n        return self.autoencoder.evaluate([X, 1.0], X, verbose=0)\n\n    def transform(self, X):\n        return self.sess.run(self.hidden, feed_dict={self.x: X, self.keep_prob: 1.0})\n\n    def generate(self, hidden=None):\n        if hidden is None:\n            hidden = np.random.normal(size=[1, self.n_hidden])\n        return self.sess.run(self.reconstruction, feed_dict={self.hidden: hidden})\n\n    def reconstruct(self, X):\n        return self.autoencoder.predict([X, 1.0])\n\n    def getWeights(self):\n        return self.weights['w1'].numpy()\n\n    def getBiases(self):\n        return self.weights['b1'].numpy()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:29:24.967436Z","iopub.execute_input":"2024-06-29T19:29:24.968037Z","iopub.status.idle":"2024-06-29T19:29:24.996452Z","shell.execute_reply.started":"2024-06-29T19:29:24.967989Z","shell.execute_reply":"2024-06-29T19:29:24.994880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing as prep\nimport csv\n\nclass Autoencoder(tf.keras.Model):\n    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, transfer_function=tf.nn.softplus,\n                 optimizer=tf.keras.optimizers.Adam()):\n        super(Autoencoder, self).__init__()\n        self.n_input = n_input\n        self.n_hidden1 = n_hidden1\n        self.n_hidden2 = n_hidden2\n        self.n_hidden3 = n_hidden3\n        self.transfer = transfer_function\n\n        # Encoder layers\n        self.hidden1 = tf.keras.layers.Dense(self.n_hidden1, activation=self.transfer)\n        self.hidden2 = tf.keras.layers.Dense(self.n_hidden2, activation=self.transfer)\n        self.hidden3 = tf.keras.layers.Dense(self.n_hidden3, activation=self.transfer)\n\n        # Decoder layers\n        self.reconstruction = tf.keras.layers.Dense(self.n_input)\n\n        # Optimizer\n        self.optimizer = optimizer\n\n    def call(self, inputs):\n        # Encoder\n        encoded = self.hidden1(inputs)\n        encoded = self.hidden2(encoded)\n        encoded = self.hidden3(encoded)\n\n        # Decoder\n        reconstructed = self.reconstruction(encoded)\n        return reconstructed\n\n# Helper function to standard scale data\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n# Helper function to get random block from data\ndef get_random_block_from_data(data, batch_size):\n    shape = tf.shape(data)\n    start_index = tf.random.uniform(shape=[], minval=0, maxval=shape[0] - batch_size, dtype=tf.int32)\n    return data[start_index:(start_index + batch_size)]\n\n# Paths and data loading (adjust paths accordingly)\npath1 = '/kaggle/input/ace-dataset/features/'\nAAC = pd.read_csv(path1 + 'ACE_AAC.csv').iloc[:, 1:]\nASDC = pd.read_csv(path1 + 'ACE_ASDC.csv').iloc[:, 1:]\nOPF_7bit_type_1 = pd.read_csv(path1 + 'opf_7bit_type_1_features.csv').iloc[1:, 1:]\nOPF_7bit_type_2 = pd.read_csv(path1 + 'opf_7bit_type_2_features.csv').iloc[1:, 1:]\nOPF_7bit_type_3 = pd.read_csv(path1 + 'opf_7bit_type_3_features.csv').iloc[1:, 1:]\nOPF_10bit = pd.read_csv(path1 + 'opf_10bit_features.csv').iloc[1:, 1:]\nesmv1 = pd.read_csv(path1 + 'esmv1_feat_ACE.csv').iloc[:, :]\nesm2 = pd.read_csv(path1 + 'esm2_t6_8M_feat_ACE.csv').iloc[:, :]\nprot_t5 = pd.read_csv(path1 + 'ACE_embeddings_prot_t5_xl_bfd.csv').iloc[1:, 1:]\n\n# Concatenate all features into a single array\nall_feat = np.column_stack((AAC, ASDC, OPF_7bit_type_1, OPF_7bit_type_2, OPF_7bit_type_3, OPF_10bit, esmv1, esm2, prot_t5))\n\n# Standard scale the data\nX_train, _ = standard_scale(all_feat, all_feat)\n\n# Define parameters\nbs = X_train.shape[0] - 1\nnum = X_train.shape[1]\nn_samples, _ = np.shape(X_train)\ntraining_epochs = 1000\nbatch_size = bs\ndisplay_step = 1\n\n# Initialize and train autoencoder\nautoencoder = Autoencoder(\n    n_input=num,\n    n_hidden1=800,\n    n_hidden2=200,\n    n_hidden3=800,\n    transfer_function=tf.nn.softplus,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n\n# Convert X_train to TensorFlow tensor\nX_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    \n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train_tf, batch_size)\n        \n        # Fit training using batch data\n        with tf.GradientTape() as tape:\n            reconstruction = autoencoder(batch_xs)\n            loss = tf.reduce_mean(tf.square(batch_xs - reconstruction))\n        \n        gradients = tape.gradient(loss, autoencoder.trainable_variables)\n        autoencoder.optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n        \n        # Compute average loss\n        avg_cost += loss / n_samples * batch_size\n    \n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%d,' % (epoch + 1),\n              \"Cost:\", \"{:.9f}\".format(avg_cost.numpy()))\n\n# Assuming your `autoencoder` model has been defined and trained\n\n# Compile the model\nautoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')  # Adjust optimizer and loss function as needed\n\n# Evaluate the loss on the training set\nloss = autoencoder.evaluate(X_train_tf, X_train_tf)\n\nprint(\"Total loss: \", loss)\n\n# Transform and reconstruct the data\nX_test_reconstruct = autoencoder.predict(X_train_tf)\n\n# Save reconstructed data to CSV\nimport csv\n\nwith open('ting_auto.csv', 'w', newline='') as fout:\n    writer = csv.writer(fout, delimiter=',')\n    for i in X_test_reconstruct:\n        writer.writerow(i)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-29T19:47:26.865399Z","iopub.execute_input":"2024-06-29T19:47:26.865759Z","iopub.status.idle":"2024-06-29T19:48:14.863377Z","shell.execute_reply.started":"2024-06-29T19:47:26.865729Z","shell.execute_reply":"2024-06-29T19:48:14.862343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing as prep\nimport csv\n\nclass Autoencoder(tf.keras.Model):\n    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, transfer_function=tf.nn.softplus,\n                 optimizer=tf.keras.optimizers.Adam()):\n        super(Autoencoder, self).__init__()\n        self.n_input = n_input\n        self.n_hidden1 = n_hidden1\n        self.n_hidden2 = n_hidden2\n        self.n_hidden3 = n_hidden3\n        self.transfer = transfer_function\n        \n        # Ensure n_hidden2 is smaller than n_input\n        assert n_hidden2 < n_input, \"n_hidden2 must be smaller than n_input\"\n\n        # Encoder layers\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.n_hidden1, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_hidden2, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_hidden3, activation=self.transfer)\n        ])\n\n        # Decoder layers\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.n_hidden2, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_hidden1, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_input, activation='linear')\n        ])\n\n        # Optimizer\n        self.optimizer = optimizer\n\n    def call(self, inputs):\n        encoded = self.encoder(inputs)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Helper function to standard scale data\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n# Paths and data loading (adjust paths accordingly)\npath1 = '/kaggle/input/ace-dataset/features/'\nAAC = pd.read_csv(path1 + 'ACE_AAC.csv').iloc[:, 1:]\nASDC = pd.read_csv(path1 + 'ACE_ASDC.csv').iloc[:, 1:]\nOPF_7bit_type_1 = pd.read_csv(path1 + 'opf_7bit_type_1_features.csv').iloc[1:, 1:]\nOPF_7bit_type_2 = pd.read_csv(path1 + 'opf_7bit_type_2_features.csv').iloc[1:, 1:]\nOPF_7bit_type_3 = pd.read_csv(path1 + 'opf_7bit_type_3_features.csv').iloc[1:, 1:]\nOPF_10bit = pd.read_csv(path1 + 'opf_10bit_features.csv').iloc[1:, 1:]\nesmv1 = pd.read_csv(path1 + 'esmv1_feat_ACE.csv').iloc[:, :]\nesm2 = pd.read_csv(path1 + 'esm2_t6_8M_feat_ACE.csv').iloc[:, :]\nprot_t5 = pd.read_csv(path1 + 'ACE_embeddings_prot_t5_xl_bfd.csv').iloc[1:, 1:]\n\n# Concatenate all features into a single array\nall_feat = np.column_stack((AAC, ASDC, OPF_7bit_type_1, OPF_7bit_type_2, OPF_7bit_type_3, OPF_10bit, esmv1, esm2, prot_t5))\n\n# Standard scale the data\nX_train, _ = standard_scale(all_feat, all_feat)\n\n# Define parameters\nnum_features = X_train.shape[1]\nn_samples, _ = np.shape(X_train)\ntraining_epochs = 1000\nbatch_size = X_train.shape[0] - 1\ndisplay_step = 1\n\n# Initialize and train autoencoder\nautoencoder = Autoencoder(\n    n_input=num_features,\n    n_hidden1=800,\n    n_hidden2=200,\n    n_hidden3=800,\n    transfer_function=tf.nn.softplus,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n\n# Convert X_train to TensorFlow tensor\nX_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    \n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = X_train_tf[i * batch_size:(i + 1) * batch_size]\n        \n        # Fit training using batch data\n        with tf.GradientTape() as tape:\n            reconstruction = autoencoder(batch_xs)\n            loss = tf.reduce_mean(tf.square(batch_xs - reconstruction))\n        \n        gradients = tape.gradient(loss, autoencoder.trainable_variables)\n        autoencoder.optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n        \n        # Compute average loss\n        avg_cost += loss / n_samples * batch_size\n    \n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%d,' % (epoch + 1),\n              \"Cost:\", \"{:.9f}\".format(avg_cost.numpy()))\n\n# Compile the model\nautoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n\n# Evaluate the loss on the training set\nloss = autoencoder.evaluate(X_train_tf, X_train_tf)\nprint(\"Total loss: \", loss)\n\n# Transform and reconstruct the data\nX_test_reconstruct = autoencoder.predict(X_train_tf)\n\n# Save reconstructed data to CSV\nwith open('ting_auto.csv', 'w', newline='') as fout:\n    writer = csv.writer(fout, delimiter=',')\n    for i in X_test_reconstruct:\n        writer.writerow(i)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:48:07.717703Z","iopub.execute_input":"2024-06-30T08:48:07.718085Z","iopub.status.idle":"2024-06-30T08:49:27.819843Z","shell.execute_reply.started":"2024-06-30T08:48:07.718053Z","shell.execute_reply":"2024-06-30T08:49:27.818787Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-30 08:48:09.521747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-30 08:48:09.521884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-30 08:48:09.676266: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Cost: 1.191434026\nEpoch: 2, Cost: 1.040842652\nEpoch: 3, Cost: 1.022527575\nEpoch: 4, Cost: 1.016745567\nEpoch: 5, Cost: 1.004657269\nEpoch: 6, Cost: 0.987540066\nEpoch: 7, Cost: 0.973439813\nEpoch: 8, Cost: 0.963098109\nEpoch: 9, Cost: 0.955717266\nEpoch: 10, Cost: 0.950278699\nEpoch: 11, Cost: 0.945162535\nEpoch: 12, Cost: 0.938891172\nEpoch: 13, Cost: 0.931070387\nEpoch: 14, Cost: 0.920981169\nEpoch: 15, Cost: 0.909448683\nEpoch: 16, Cost: 0.897775710\nEpoch: 17, Cost: 0.886345387\nEpoch: 18, Cost: 0.874743521\nEpoch: 19, Cost: 0.863596737\nEpoch: 20, Cost: 0.853292584\nEpoch: 21, Cost: 0.843355954\nEpoch: 22, Cost: 0.833392143\nEpoch: 23, Cost: 0.823311210\nEpoch: 24, Cost: 0.812482119\nEpoch: 25, Cost: 0.800881565\nEpoch: 26, Cost: 0.788570225\nEpoch: 27, Cost: 0.775662363\nEpoch: 28, Cost: 0.762386322\nEpoch: 29, Cost: 0.749638319\nEpoch: 30, Cost: 0.736678123\nEpoch: 31, Cost: 0.720449626\nEpoch: 32, Cost: 0.705861747\nEpoch: 33, Cost: 0.694787860\nEpoch: 34, Cost: 0.681813061\nEpoch: 35, Cost: 0.667571068\nEpoch: 36, Cost: 0.657079875\nEpoch: 37, Cost: 0.646309435\nEpoch: 38, Cost: 0.633131504\nEpoch: 39, Cost: 0.622599244\nEpoch: 40, Cost: 0.613239348\nEpoch: 41, Cost: 0.602665782\nEpoch: 42, Cost: 0.595478594\nEpoch: 43, Cost: 0.591053128\nEpoch: 44, Cost: 0.576424003\nEpoch: 45, Cost: 0.567522109\nEpoch: 46, Cost: 0.562152147\nEpoch: 47, Cost: 0.550058544\nEpoch: 48, Cost: 0.545051098\nEpoch: 49, Cost: 0.534488976\nEpoch: 50, Cost: 0.528862536\nEpoch: 51, Cost: 0.520644724\nEpoch: 52, Cost: 0.512376010\nEpoch: 53, Cost: 0.507538438\nEpoch: 54, Cost: 0.499516129\nEpoch: 55, Cost: 0.495707810\nEpoch: 56, Cost: 0.487303793\nEpoch: 57, Cost: 0.482081503\nEpoch: 58, Cost: 0.473809451\nEpoch: 59, Cost: 0.469356656\nEpoch: 60, Cost: 0.464253128\nEpoch: 61, Cost: 0.458572298\nEpoch: 62, Cost: 0.453714520\nEpoch: 63, Cost: 0.447537750\nEpoch: 64, Cost: 0.444084257\nEpoch: 65, Cost: 0.438312024\nEpoch: 66, Cost: 0.434791535\nEpoch: 67, Cost: 0.429246783\nEpoch: 68, Cost: 0.425281852\nEpoch: 69, Cost: 0.420875967\nEpoch: 70, Cost: 0.416052461\nEpoch: 71, Cost: 0.412890255\nEpoch: 72, Cost: 0.408287883\nEpoch: 73, Cost: 0.404785901\nEpoch: 74, Cost: 0.400293142\nEpoch: 75, Cost: 0.396097869\nEpoch: 76, Cost: 0.392901480\nEpoch: 77, Cost: 0.389366716\nEpoch: 78, Cost: 0.387531161\nEpoch: 79, Cost: 0.387364537\nEpoch: 80, Cost: 0.389113426\nEpoch: 81, Cost: 0.383941054\nEpoch: 82, Cost: 0.374286860\nEpoch: 83, Cost: 0.373940289\nEpoch: 84, Cost: 0.368849903\nEpoch: 85, Cost: 0.364296287\nEpoch: 86, Cost: 0.363048702\nEpoch: 87, Cost: 0.357225865\nEpoch: 88, Cost: 0.356296867\nEpoch: 89, Cost: 0.351686627\nEpoch: 90, Cost: 0.349138469\nEpoch: 91, Cost: 0.345964909\nEpoch: 92, Cost: 0.343278199\nEpoch: 93, Cost: 0.339866489\nEpoch: 94, Cost: 0.337257922\nEpoch: 95, Cost: 0.334692538\nEpoch: 96, Cost: 0.331107765\nEpoch: 97, Cost: 0.329471827\nEpoch: 98, Cost: 0.325527340\nEpoch: 99, Cost: 0.323981404\nEpoch: 100, Cost: 0.320728570\nEpoch: 101, Cost: 0.318384349\nEpoch: 102, Cost: 0.316697329\nEpoch: 103, Cost: 0.314412534\nEpoch: 104, Cost: 0.315063894\nEpoch: 105, Cost: 0.316499770\nEpoch: 106, Cost: 0.314175397\nEpoch: 107, Cost: 0.306866735\nEpoch: 108, Cost: 0.303084850\nEpoch: 109, Cost: 0.304955333\nEpoch: 110, Cost: 0.300189972\nEpoch: 111, Cost: 0.296744913\nEpoch: 112, Cost: 0.296819121\nEpoch: 113, Cost: 0.292826533\nEpoch: 114, Cost: 0.290405273\nEpoch: 115, Cost: 0.288986892\nEpoch: 116, Cost: 0.285600245\nEpoch: 117, Cost: 0.284314126\nEpoch: 118, Cost: 0.282237947\nEpoch: 119, Cost: 0.279626906\nEpoch: 120, Cost: 0.278823227\nEpoch: 121, Cost: 0.276333749\nEpoch: 122, Cost: 0.274987429\nEpoch: 123, Cost: 0.273942173\nEpoch: 124, Cost: 0.272292584\nEpoch: 125, Cost: 0.272086769\nEpoch: 126, Cost: 0.270518959\nEpoch: 127, Cost: 0.268214762\nEpoch: 128, Cost: 0.265271783\nEpoch: 129, Cost: 0.262318969\nEpoch: 130, Cost: 0.262191325\nEpoch: 131, Cost: 0.261055410\nEpoch: 132, Cost: 0.258875817\nEpoch: 133, Cost: 0.256089479\nEpoch: 134, Cost: 0.254494071\nEpoch: 135, Cost: 0.254150778\nEpoch: 136, Cost: 0.252295941\nEpoch: 137, Cost: 0.250703275\nEpoch: 138, Cost: 0.250471622\nEpoch: 139, Cost: 0.250305504\nEpoch: 140, Cost: 0.250098467\nEpoch: 141, Cost: 0.246969193\nEpoch: 142, Cost: 0.244542643\nEpoch: 143, Cost: 0.243005306\nEpoch: 144, Cost: 0.241547689\nEpoch: 145, Cost: 0.240546644\nEpoch: 146, Cost: 0.238995641\nEpoch: 147, Cost: 0.237459004\nEpoch: 148, Cost: 0.235295430\nEpoch: 149, Cost: 0.234186247\nEpoch: 150, Cost: 0.233745337\nEpoch: 151, Cost: 0.231571048\nEpoch: 152, Cost: 0.230325937\nEpoch: 153, Cost: 0.230082557\nEpoch: 154, Cost: 0.228552610\nEpoch: 155, Cost: 0.227143556\nEpoch: 156, Cost: 0.226028576\nEpoch: 157, Cost: 0.224926457\nEpoch: 158, Cost: 0.224447355\nEpoch: 159, Cost: 0.224938735\nEpoch: 160, Cost: 0.225231111\nEpoch: 161, Cost: 0.222074226\nEpoch: 162, Cost: 0.218630224\nEpoch: 163, Cost: 0.217802435\nEpoch: 164, Cost: 0.217303798\nEpoch: 165, Cost: 0.215649381\nEpoch: 166, Cost: 0.213838980\nEpoch: 167, Cost: 0.212337330\nEpoch: 168, Cost: 0.211314857\nEpoch: 169, Cost: 0.210256532\nEpoch: 170, Cost: 0.208671987\nEpoch: 171, Cost: 0.207553998\nEpoch: 172, Cost: 0.206791058\nEpoch: 173, Cost: 0.205348164\nEpoch: 174, Cost: 0.204097211\nEpoch: 175, Cost: 0.203622475\nEpoch: 176, Cost: 0.202577978\nEpoch: 177, Cost: 0.201737508\nEpoch: 178, Cost: 0.202154949\nEpoch: 179, Cost: 0.202609181\nEpoch: 180, Cost: 0.202533945\nEpoch: 181, Cost: 0.202101871\nEpoch: 182, Cost: 0.198299527\nEpoch: 183, Cost: 0.195283666\nEpoch: 184, Cost: 0.195413470\nEpoch: 185, Cost: 0.195437267\nEpoch: 186, Cost: 0.193252802\nEpoch: 187, Cost: 0.191556901\nEpoch: 188, Cost: 0.191677257\nEpoch: 189, Cost: 0.191003844\nEpoch: 190, Cost: 0.189470902\nEpoch: 191, Cost: 0.188665837\nEpoch: 192, Cost: 0.188325524\nEpoch: 193, Cost: 0.187667429\nEpoch: 194, Cost: 0.186644897\nEpoch: 195, Cost: 0.186043561\nEpoch: 196, Cost: 0.185975030\nEpoch: 197, Cost: 0.186464638\nEpoch: 198, Cost: 0.186246067\nEpoch: 199, Cost: 0.186571002\nEpoch: 200, Cost: 0.184800580\nEpoch: 201, Cost: 0.182594851\nEpoch: 202, Cost: 0.184516758\nEpoch: 203, Cost: 0.185820088\nEpoch: 204, Cost: 0.182279438\nEpoch: 205, Cost: 0.179520264\nEpoch: 206, Cost: 0.177691787\nEpoch: 207, Cost: 0.176242128\nEpoch: 208, Cost: 0.176440015\nEpoch: 209, Cost: 0.174617648\nEpoch: 210, Cost: 0.173301995\nEpoch: 211, Cost: 0.172386631\nEpoch: 212, Cost: 0.171082541\nEpoch: 213, Cost: 0.170698076\nEpoch: 214, Cost: 0.169018641\nEpoch: 215, Cost: 0.168205783\nEpoch: 216, Cost: 0.167924970\nEpoch: 217, Cost: 0.166091025\nEpoch: 218, Cost: 0.165754288\nEpoch: 219, Cost: 0.165198624\nEpoch: 220, Cost: 0.163805813\nEpoch: 221, Cost: 0.163762346\nEpoch: 222, Cost: 0.163254246\nEpoch: 223, Cost: 0.163192004\nEpoch: 224, Cost: 0.164082333\nEpoch: 225, Cost: 0.165677205\nEpoch: 226, Cost: 0.166808128\nEpoch: 227, Cost: 0.166142002\nEpoch: 228, Cost: 0.160502151\nEpoch: 229, Cost: 0.157797918\nEpoch: 230, Cost: 0.159415230\nEpoch: 231, Cost: 0.159492612\nEpoch: 232, Cost: 0.156663358\nEpoch: 233, Cost: 0.154914215\nEpoch: 234, Cost: 0.156200409\nEpoch: 235, Cost: 0.154938996\nEpoch: 236, Cost: 0.152523443\nEpoch: 237, Cost: 0.153501779\nEpoch: 238, Cost: 0.153192803\nEpoch: 239, Cost: 0.151534393\nEpoch: 240, Cost: 0.151916042\nEpoch: 241, Cost: 0.152410701\nEpoch: 242, Cost: 0.151674688\nEpoch: 243, Cost: 0.151637629\nEpoch: 244, Cost: 0.150653377\nEpoch: 245, Cost: 0.148565561\nEpoch: 246, Cost: 0.146786615\nEpoch: 247, Cost: 0.146551669\nEpoch: 248, Cost: 0.146909207\nEpoch: 249, Cost: 0.146406084\nEpoch: 250, Cost: 0.145690769\nEpoch: 251, Cost: 0.144795537\nEpoch: 252, Cost: 0.144788340\nEpoch: 253, Cost: 0.144992352\nEpoch: 254, Cost: 0.144900993\nEpoch: 255, Cost: 0.143704057\nEpoch: 256, Cost: 0.142673761\nEpoch: 257, Cost: 0.142535254\nEpoch: 258, Cost: 0.143378496\nEpoch: 259, Cost: 0.143377945\nEpoch: 260, Cost: 0.143191874\nEpoch: 261, Cost: 0.142637163\nEpoch: 262, Cost: 0.141880631\nEpoch: 263, Cost: 0.141752660\nEpoch: 264, Cost: 0.140853554\nEpoch: 265, Cost: 0.138979897\nEpoch: 266, Cost: 0.137112707\nEpoch: 267, Cost: 0.136113629\nEpoch: 268, Cost: 0.136678934\nEpoch: 269, Cost: 0.136102736\nEpoch: 270, Cost: 0.134089589\nEpoch: 271, Cost: 0.133959353\nEpoch: 272, Cost: 0.133614987\nEpoch: 273, Cost: 0.132112071\nEpoch: 274, Cost: 0.131432399\nEpoch: 275, Cost: 0.130985439\nEpoch: 276, Cost: 0.130217418\nEpoch: 277, Cost: 0.129608244\nEpoch: 278, Cost: 0.128979146\nEpoch: 279, Cost: 0.128399879\nEpoch: 280, Cost: 0.127923071\nEpoch: 281, Cost: 0.127260268\nEpoch: 282, Cost: 0.126724094\nEpoch: 283, Cost: 0.126434773\nEpoch: 284, Cost: 0.125943676\nEpoch: 285, Cost: 0.125633731\nEpoch: 286, Cost: 0.125849068\nEpoch: 287, Cost: 0.126649424\nEpoch: 288, Cost: 0.128449887\nEpoch: 289, Cost: 0.131485730\nEpoch: 290, Cost: 0.134037122\nEpoch: 291, Cost: 0.131385520\nEpoch: 292, Cost: 0.124780051\nEpoch: 293, Cost: 0.123966619\nEpoch: 294, Cost: 0.128008485\nEpoch: 295, Cost: 0.126393139\nEpoch: 296, Cost: 0.122204229\nEpoch: 297, Cost: 0.124015354\nEpoch: 298, Cost: 0.124055475\nEpoch: 299, Cost: 0.121210776\nEpoch: 300, Cost: 0.121715829\nEpoch: 301, Cost: 0.120799914\nEpoch: 302, Cost: 0.119359352\nEpoch: 303, Cost: 0.120146617\nEpoch: 304, Cost: 0.119669385\nEpoch: 305, Cost: 0.119512133\nEpoch: 306, Cost: 0.119694971\nEpoch: 307, Cost: 0.118226960\nEpoch: 308, Cost: 0.117559165\nEpoch: 309, Cost: 0.116788864\nEpoch: 310, Cost: 0.115663022\nEpoch: 311, Cost: 0.115366176\nEpoch: 312, Cost: 0.114958636\nEpoch: 313, Cost: 0.114525177\nEpoch: 314, Cost: 0.114578292\nEpoch: 315, Cost: 0.114390194\nEpoch: 316, Cost: 0.114227109\nEpoch: 317, Cost: 0.114525899\nEpoch: 318, Cost: 0.114931457\nEpoch: 319, Cost: 0.115470655\nEpoch: 320, Cost: 0.115315184\nEpoch: 321, Cost: 0.114379421\nEpoch: 322, Cost: 0.112359613\nEpoch: 323, Cost: 0.111028269\nEpoch: 324, Cost: 0.111551777\nEpoch: 325, Cost: 0.112418488\nEpoch: 326, Cost: 0.112099454\nEpoch: 327, Cost: 0.111364163\nEpoch: 328, Cost: 0.110816889\nEpoch: 329, Cost: 0.111224994\nEpoch: 330, Cost: 0.109686844\nEpoch: 331, Cost: 0.107921019\nEpoch: 332, Cost: 0.107346222\nEpoch: 333, Cost: 0.107929945\nEpoch: 334, Cost: 0.107855588\nEpoch: 335, Cost: 0.106460400\nEpoch: 336, Cost: 0.105872147\nEpoch: 337, Cost: 0.106272623\nEpoch: 338, Cost: 0.105924100\nEpoch: 339, Cost: 0.105318785\nEpoch: 340, Cost: 0.105682112\nEpoch: 341, Cost: 0.107531451\nEpoch: 342, Cost: 0.110532992\nEpoch: 343, Cost: 0.112531818\nEpoch: 344, Cost: 0.111589283\nEpoch: 345, Cost: 0.106249385\nEpoch: 346, Cost: 0.103580371\nEpoch: 347, Cost: 0.105941348\nEpoch: 348, Cost: 0.106212266\nEpoch: 349, Cost: 0.102811366\nEpoch: 350, Cost: 0.102836460\nEpoch: 351, Cost: 0.103312664\nEpoch: 352, Cost: 0.101584874\nEpoch: 353, Cost: 0.101574764\nEpoch: 354, Cost: 0.101552278\nEpoch: 355, Cost: 0.100329623\nEpoch: 356, Cost: 0.100824334\nEpoch: 357, Cost: 0.100915939\nEpoch: 358, Cost: 0.100065798\nEpoch: 359, Cost: 0.101222225\nEpoch: 360, Cost: 0.102431536\nEpoch: 361, Cost: 0.102426484\nEpoch: 362, Cost: 0.102597475\nEpoch: 363, Cost: 0.100741766\nEpoch: 364, Cost: 0.098037988\nEpoch: 365, Cost: 0.097765997\nEpoch: 366, Cost: 0.098676763\nEpoch: 367, Cost: 0.098412924\nEpoch: 368, Cost: 0.097136341\nEpoch: 369, Cost: 0.095765136\nEpoch: 370, Cost: 0.095954277\nEpoch: 371, Cost: 0.096707433\nEpoch: 372, Cost: 0.096195512\nEpoch: 373, Cost: 0.095732793\nEpoch: 374, Cost: 0.096780665\nEpoch: 375, Cost: 0.098265402\nEpoch: 376, Cost: 0.099395365\nEpoch: 377, Cost: 0.100419186\nEpoch: 378, Cost: 0.099692002\nEpoch: 379, Cost: 0.097698495\nEpoch: 380, Cost: 0.095772050\nEpoch: 381, Cost: 0.096275710\nEpoch: 382, Cost: 0.096796706\nEpoch: 383, Cost: 0.094686098\nEpoch: 384, Cost: 0.092210419\nEpoch: 385, Cost: 0.092690997\nEpoch: 386, Cost: 0.093647256\nEpoch: 387, Cost: 0.092347190\nEpoch: 388, Cost: 0.091262281\nEpoch: 389, Cost: 0.091590866\nEpoch: 390, Cost: 0.091211982\nEpoch: 391, Cost: 0.090148613\nEpoch: 392, Cost: 0.090223312\nEpoch: 393, Cost: 0.090325914\nEpoch: 394, Cost: 0.089523979\nEpoch: 395, Cost: 0.089128837\nEpoch: 396, Cost: 0.089451790\nEpoch: 397, Cost: 0.089415357\nEpoch: 398, Cost: 0.089581937\nEpoch: 399, Cost: 0.090843052\nEpoch: 400, Cost: 0.093517661\nEpoch: 401, Cost: 0.095748238\nEpoch: 402, Cost: 0.097547702\nEpoch: 403, Cost: 0.094577014\nEpoch: 404, Cost: 0.089925058\nEpoch: 405, Cost: 0.090487354\nEpoch: 406, Cost: 0.093672134\nEpoch: 407, Cost: 0.092718132\nEpoch: 408, Cost: 0.089167021\nEpoch: 409, Cost: 0.090393148\nEpoch: 410, Cost: 0.090211548\nEpoch: 411, Cost: 0.087038055\nEpoch: 412, Cost: 0.087196052\nEpoch: 413, Cost: 0.087964214\nEpoch: 414, Cost: 0.086368032\nEpoch: 415, Cost: 0.086570799\nEpoch: 416, Cost: 0.086413182\nEpoch: 417, Cost: 0.084809721\nEpoch: 418, Cost: 0.085072480\nEpoch: 419, Cost: 0.084834225\nEpoch: 420, Cost: 0.083960101\nEpoch: 421, Cost: 0.084284425\nEpoch: 422, Cost: 0.083782449\nEpoch: 423, Cost: 0.083139412\nEpoch: 424, Cost: 0.083400697\nEpoch: 425, Cost: 0.083074771\nEpoch: 426, Cost: 0.082899876\nEpoch: 427, Cost: 0.083410233\nEpoch: 428, Cost: 0.083788767\nEpoch: 429, Cost: 0.084541127\nEpoch: 430, Cost: 0.086464077\nEpoch: 431, Cost: 0.087467581\nEpoch: 432, Cost: 0.087861866\nEpoch: 433, Cost: 0.085275978\nEpoch: 434, Cost: 0.082160994\nEpoch: 435, Cost: 0.081873879\nEpoch: 436, Cost: 0.083896153\nEpoch: 437, Cost: 0.084978007\nEpoch: 438, Cost: 0.082885258\nEpoch: 439, Cost: 0.082232818\nEpoch: 440, Cost: 0.083815195\nEpoch: 441, Cost: 0.084515162\nEpoch: 442, Cost: 0.082330845\nEpoch: 443, Cost: 0.080703355\nEpoch: 444, Cost: 0.080915093\nEpoch: 445, Cost: 0.080786452\nEpoch: 446, Cost: 0.079892680\nEpoch: 447, Cost: 0.079899520\nEpoch: 448, Cost: 0.079970069\nEpoch: 449, Cost: 0.078807883\nEpoch: 450, Cost: 0.078206778\nEpoch: 451, Cost: 0.078802742\nEpoch: 452, Cost: 0.078664169\nEpoch: 453, Cost: 0.077804767\nEpoch: 454, Cost: 0.077598304\nEpoch: 455, Cost: 0.077929534\nEpoch: 456, Cost: 0.077972241\nEpoch: 457, Cost: 0.078298166\nEpoch: 458, Cost: 0.079490177\nEpoch: 459, Cost: 0.081775993\nEpoch: 460, Cost: 0.083490327\nEpoch: 461, Cost: 0.085621171\nEpoch: 462, Cost: 0.084269285\nEpoch: 463, Cost: 0.081695952\nEpoch: 464, Cost: 0.080434538\nEpoch: 465, Cost: 0.082193732\nEpoch: 466, Cost: 0.080679700\nEpoch: 467, Cost: 0.076506101\nEpoch: 468, Cost: 0.076896332\nEpoch: 469, Cost: 0.079216838\nEpoch: 470, Cost: 0.077249423\nEpoch: 471, Cost: 0.075273894\nEpoch: 472, Cost: 0.076647535\nEpoch: 473, Cost: 0.076205619\nEpoch: 474, Cost: 0.074509367\nEpoch: 475, Cost: 0.075077720\nEpoch: 476, Cost: 0.075114079\nEpoch: 477, Cost: 0.073859915\nEpoch: 478, Cost: 0.073913820\nEpoch: 479, Cost: 0.073992260\nEpoch: 480, Cost: 0.073239021\nEpoch: 481, Cost: 0.073229425\nEpoch: 482, Cost: 0.073259018\nEpoch: 483, Cost: 0.072517321\nEpoch: 484, Cost: 0.072443187\nEpoch: 485, Cost: 0.072642483\nEpoch: 486, Cost: 0.072154231\nEpoch: 487, Cost: 0.072023414\nEpoch: 488, Cost: 0.072517328\nEpoch: 489, Cost: 0.072651826\nEpoch: 490, Cost: 0.073350616\nEpoch: 491, Cost: 0.075160958\nEpoch: 492, Cost: 0.077927068\nEpoch: 493, Cost: 0.079162054\nEpoch: 494, Cost: 0.078556895\nEpoch: 495, Cost: 0.073479541\nEpoch: 496, Cost: 0.070977457\nEpoch: 497, Cost: 0.073530979\nEpoch: 498, Cost: 0.074508049\nEpoch: 499, Cost: 0.071956426\nEpoch: 500, Cost: 0.071331464\nEpoch: 501, Cost: 0.073564537\nEpoch: 502, Cost: 0.072966740\nEpoch: 503, Cost: 0.071500912\nEpoch: 504, Cost: 0.072914936\nEpoch: 505, Cost: 0.072914734\nEpoch: 506, Cost: 0.070790753\nEpoch: 507, Cost: 0.070996530\nEpoch: 508, Cost: 0.070557132\nEpoch: 509, Cost: 0.069291964\nEpoch: 510, Cost: 0.070212565\nEpoch: 511, Cost: 0.070847742\nEpoch: 512, Cost: 0.070730545\nEpoch: 513, Cost: 0.071766190\nEpoch: 514, Cost: 0.072300576\nEpoch: 515, Cost: 0.071761936\nEpoch: 516, Cost: 0.072076045\nEpoch: 517, Cost: 0.071642585\nEpoch: 518, Cost: 0.071058042\nEpoch: 519, Cost: 0.071849667\nEpoch: 520, Cost: 0.072375081\nEpoch: 521, Cost: 0.072828799\nEpoch: 522, Cost: 0.073597342\nEpoch: 523, Cost: 0.072970934\nEpoch: 524, Cost: 0.072019517\nEpoch: 525, Cost: 0.070131041\nEpoch: 526, Cost: 0.068833284\nEpoch: 527, Cost: 0.069209941\nEpoch: 528, Cost: 0.069869310\nEpoch: 529, Cost: 0.068939984\nEpoch: 530, Cost: 0.066992886\nEpoch: 531, Cost: 0.066957735\nEpoch: 532, Cost: 0.067940816\nEpoch: 533, Cost: 0.067663804\nEpoch: 534, Cost: 0.066530004\nEpoch: 535, Cost: 0.066179238\nEpoch: 536, Cost: 0.066620879\nEpoch: 537, Cost: 0.066432089\nEpoch: 538, Cost: 0.065509252\nEpoch: 539, Cost: 0.065544583\nEpoch: 540, Cost: 0.066139236\nEpoch: 541, Cost: 0.066095032\nEpoch: 542, Cost: 0.065899760\nEpoch: 543, Cost: 0.066729903\nEpoch: 544, Cost: 0.068324402\nEpoch: 545, Cost: 0.070318490\nEpoch: 546, Cost: 0.070968963\nEpoch: 547, Cost: 0.070765994\nEpoch: 548, Cost: 0.067891151\nEpoch: 549, Cost: 0.065301985\nEpoch: 550, Cost: 0.065793760\nEpoch: 551, Cost: 0.067462347\nEpoch: 552, Cost: 0.066969603\nEpoch: 553, Cost: 0.064792417\nEpoch: 554, Cost: 0.064877503\nEpoch: 555, Cost: 0.066055439\nEpoch: 556, Cost: 0.065374069\nEpoch: 557, Cost: 0.064251900\nEpoch: 558, Cost: 0.064990811\nEpoch: 559, Cost: 0.065304250\nEpoch: 560, Cost: 0.064405620\nEpoch: 561, Cost: 0.064166673\nEpoch: 562, Cost: 0.065091774\nEpoch: 563, Cost: 0.064939588\nEpoch: 564, Cost: 0.064752169\nEpoch: 565, Cost: 0.065663978\nEpoch: 566, Cost: 0.066431634\nEpoch: 567, Cost: 0.066388778\nEpoch: 568, Cost: 0.066445947\nEpoch: 569, Cost: 0.066963539\nEpoch: 570, Cost: 0.066734605\nEpoch: 571, Cost: 0.067120172\nEpoch: 572, Cost: 0.067833073\nEpoch: 573, Cost: 0.066862963\nEpoch: 574, Cost: 0.064698778\nEpoch: 575, Cost: 0.063910156\nEpoch: 576, Cost: 0.064476781\nEpoch: 577, Cost: 0.063552901\nEpoch: 578, Cost: 0.062452879\nEpoch: 579, Cost: 0.062437516\nEpoch: 580, Cost: 0.062742770\nEpoch: 581, Cost: 0.062488742\nEpoch: 582, Cost: 0.061407689\nEpoch: 583, Cost: 0.060972411\nEpoch: 584, Cost: 0.061476476\nEpoch: 585, Cost: 0.061502147\nEpoch: 586, Cost: 0.060711302\nEpoch: 587, Cost: 0.060333323\nEpoch: 588, Cost: 0.060718689\nEpoch: 589, Cost: 0.060799174\nEpoch: 590, Cost: 0.060216442\nEpoch: 591, Cost: 0.060139690\nEpoch: 592, Cost: 0.060653903\nEpoch: 593, Cost: 0.060970712\nEpoch: 594, Cost: 0.061232869\nEpoch: 595, Cost: 0.062115125\nEpoch: 596, Cost: 0.063476525\nEpoch: 597, Cost: 0.064484142\nEpoch: 598, Cost: 0.064145155\nEpoch: 599, Cost: 0.062639199\nEpoch: 600, Cost: 0.061041132\nEpoch: 601, Cost: 0.060564555\nEpoch: 602, Cost: 0.061577458\nEpoch: 603, Cost: 0.062094498\nEpoch: 604, Cost: 0.061187129\nEpoch: 605, Cost: 0.059692841\nEpoch: 606, Cost: 0.059509892\nEpoch: 607, Cost: 0.060020696\nEpoch: 608, Cost: 0.059889369\nEpoch: 609, Cost: 0.059003998\nEpoch: 610, Cost: 0.058911595\nEpoch: 611, Cost: 0.059391465\nEpoch: 612, Cost: 0.059435096\nEpoch: 613, Cost: 0.058969356\nEpoch: 614, Cost: 0.058819626\nEpoch: 615, Cost: 0.059173770\nEpoch: 616, Cost: 0.059292361\nEpoch: 617, Cost: 0.059087507\nEpoch: 618, Cost: 0.058949579\nEpoch: 619, Cost: 0.059232097\nEpoch: 620, Cost: 0.059330221\nEpoch: 621, Cost: 0.059187580\nEpoch: 622, Cost: 0.058941588\nEpoch: 623, Cost: 0.059239265\nEpoch: 624, Cost: 0.060083453\nEpoch: 625, Cost: 0.061067313\nEpoch: 626, Cost: 0.062496603\nEpoch: 627, Cost: 0.062964402\nEpoch: 628, Cost: 0.062603392\nEpoch: 629, Cost: 0.060423799\nEpoch: 630, Cost: 0.059386607\nEpoch: 631, Cost: 0.060150329\nEpoch: 632, Cost: 0.060554698\nEpoch: 633, Cost: 0.058648292\nEpoch: 634, Cost: 0.057842471\nEpoch: 635, Cost: 0.059327047\nEpoch: 636, Cost: 0.060557131\nEpoch: 637, Cost: 0.059763193\nEpoch: 638, Cost: 0.060097158\nEpoch: 639, Cost: 0.061979249\nEpoch: 640, Cost: 0.061205849\nEpoch: 641, Cost: 0.058617424\nEpoch: 642, Cost: 0.057270121\nEpoch: 643, Cost: 0.057135817\nEpoch: 644, Cost: 0.056895670\nEpoch: 645, Cost: 0.057091612\nEpoch: 646, Cost: 0.057463318\nEpoch: 647, Cost: 0.056461863\nEpoch: 648, Cost: 0.055480529\nEpoch: 649, Cost: 0.055809665\nEpoch: 650, Cost: 0.056079119\nEpoch: 651, Cost: 0.055623423\nEpoch: 652, Cost: 0.055425540\nEpoch: 653, Cost: 0.055355858\nEpoch: 654, Cost: 0.055262275\nEpoch: 655, Cost: 0.055460405\nEpoch: 656, Cost: 0.055930410\nEpoch: 657, Cost: 0.055947572\nEpoch: 658, Cost: 0.056299116\nEpoch: 659, Cost: 0.056874506\nEpoch: 660, Cost: 0.057588633\nEpoch: 661, Cost: 0.057604823\nEpoch: 662, Cost: 0.057166658\nEpoch: 663, Cost: 0.055811454\nEpoch: 664, Cost: 0.054607969\nEpoch: 665, Cost: 0.054566439\nEpoch: 666, Cost: 0.055358220\nEpoch: 667, Cost: 0.056201581\nEpoch: 668, Cost: 0.055993147\nEpoch: 669, Cost: 0.055665206\nEpoch: 670, Cost: 0.054909855\nEpoch: 671, Cost: 0.054862041\nEpoch: 672, Cost: 0.055248074\nEpoch: 673, Cost: 0.055353850\nEpoch: 674, Cost: 0.055087812\nEpoch: 675, Cost: 0.054772448\nEpoch: 676, Cost: 0.055069011\nEpoch: 677, Cost: 0.055334970\nEpoch: 678, Cost: 0.055262215\nEpoch: 679, Cost: 0.054677974\nEpoch: 680, Cost: 0.054402713\nEpoch: 681, Cost: 0.054497365\nEpoch: 682, Cost: 0.055043139\nEpoch: 683, Cost: 0.055383928\nEpoch: 684, Cost: 0.055348806\nEpoch: 685, Cost: 0.055125605\nEpoch: 686, Cost: 0.054761712\nEpoch: 687, Cost: 0.054276217\nEpoch: 688, Cost: 0.053867716\nEpoch: 689, Cost: 0.053967968\nEpoch: 690, Cost: 0.054630775\nEpoch: 691, Cost: 0.054731164\nEpoch: 692, Cost: 0.054337885\nEpoch: 693, Cost: 0.053695668\nEpoch: 694, Cost: 0.053732369\nEpoch: 695, Cost: 0.054176141\nEpoch: 696, Cost: 0.054289442\nEpoch: 697, Cost: 0.053752031\nEpoch: 698, Cost: 0.053166088\nEpoch: 699, Cost: 0.053035874\nEpoch: 700, Cost: 0.053306282\nEpoch: 701, Cost: 0.053237218\nEpoch: 702, Cost: 0.052895010\nEpoch: 703, Cost: 0.052557014\nEpoch: 704, Cost: 0.052366000\nEpoch: 705, Cost: 0.052394364\nEpoch: 706, Cost: 0.052519772\nEpoch: 707, Cost: 0.052639008\nEpoch: 708, Cost: 0.052758988\nEpoch: 709, Cost: 0.052652951\nEpoch: 710, Cost: 0.052849680\nEpoch: 711, Cost: 0.053138245\nEpoch: 712, Cost: 0.053637605\nEpoch: 713, Cost: 0.053908952\nEpoch: 714, Cost: 0.054087300\nEpoch: 715, Cost: 0.053999625\nEpoch: 716, Cost: 0.053618371\nEpoch: 717, Cost: 0.053025853\nEpoch: 718, Cost: 0.052706473\nEpoch: 719, Cost: 0.052961878\nEpoch: 720, Cost: 0.053293090\nEpoch: 721, Cost: 0.053696137\nEpoch: 722, Cost: 0.054147381\nEpoch: 723, Cost: 0.055050302\nEpoch: 724, Cost: 0.055172376\nEpoch: 725, Cost: 0.053778965\nEpoch: 726, Cost: 0.051913224\nEpoch: 727, Cost: 0.051438045\nEpoch: 728, Cost: 0.051820692\nEpoch: 729, Cost: 0.051493209\nEpoch: 730, Cost: 0.050857600\nEpoch: 731, Cost: 0.050984107\nEpoch: 732, Cost: 0.051437594\nEpoch: 733, Cost: 0.050749455\nEpoch: 734, Cost: 0.050025653\nEpoch: 735, Cost: 0.050182562\nEpoch: 736, Cost: 0.050481066\nEpoch: 737, Cost: 0.050116532\nEpoch: 738, Cost: 0.049695771\nEpoch: 739, Cost: 0.049856663\nEpoch: 740, Cost: 0.049761664\nEpoch: 741, Cost: 0.049262110\nEpoch: 742, Cost: 0.049054865\nEpoch: 743, Cost: 0.049310982\nEpoch: 744, Cost: 0.049305554\nEpoch: 745, Cost: 0.049053807\nEpoch: 746, Cost: 0.049021557\nEpoch: 747, Cost: 0.049226012\nEpoch: 748, Cost: 0.049182221\nEpoch: 749, Cost: 0.049028974\nEpoch: 750, Cost: 0.049042825\nEpoch: 751, Cost: 0.049230974\nEpoch: 752, Cost: 0.049250852\nEpoch: 753, Cost: 0.049344648\nEpoch: 754, Cost: 0.049518842\nEpoch: 755, Cost: 0.049915776\nEpoch: 756, Cost: 0.050247263\nEpoch: 757, Cost: 0.050956618\nEpoch: 758, Cost: 0.051799890\nEpoch: 759, Cost: 0.052604649\nEpoch: 760, Cost: 0.052573234\nEpoch: 761, Cost: 0.051507905\nEpoch: 762, Cost: 0.050361250\nEpoch: 763, Cost: 0.049810518\nEpoch: 764, Cost: 0.050428227\nEpoch: 765, Cost: 0.051065844\nEpoch: 766, Cost: 0.050815988\nEpoch: 767, Cost: 0.049882587\nEpoch: 768, Cost: 0.049339216\nEpoch: 769, Cost: 0.049974952\nEpoch: 770, Cost: 0.050025955\nEpoch: 771, Cost: 0.049245223\nEpoch: 772, Cost: 0.048235774\nEpoch: 773, Cost: 0.048527304\nEpoch: 774, Cost: 0.049329620\nEpoch: 775, Cost: 0.049211144\nEpoch: 776, Cost: 0.048495840\nEpoch: 777, Cost: 0.048563566\nEpoch: 778, Cost: 0.049378842\nEpoch: 779, Cost: 0.049819067\nEpoch: 780, Cost: 0.049218513\nEpoch: 781, Cost: 0.048595835\nEpoch: 782, Cost: 0.048730645\nEpoch: 783, Cost: 0.048684664\nEpoch: 784, Cost: 0.048500162\nEpoch: 785, Cost: 0.048168793\nEpoch: 786, Cost: 0.048468236\nEpoch: 787, Cost: 0.048585512\nEpoch: 788, Cost: 0.048442051\nEpoch: 789, Cost: 0.048166655\nEpoch: 790, Cost: 0.048476588\nEpoch: 791, Cost: 0.048767783\nEpoch: 792, Cost: 0.049022000\nEpoch: 793, Cost: 0.048922423\nEpoch: 794, Cost: 0.048832219\nEpoch: 795, Cost: 0.048683178\nEpoch: 796, Cost: 0.048487235\nEpoch: 797, Cost: 0.048600126\nEpoch: 798, Cost: 0.048531353\nEpoch: 799, Cost: 0.048102729\nEpoch: 800, Cost: 0.047121428\nEpoch: 801, Cost: 0.046465155\nEpoch: 802, Cost: 0.046546534\nEpoch: 803, Cost: 0.047031768\nEpoch: 804, Cost: 0.047275133\nEpoch: 805, Cost: 0.047086645\nEpoch: 806, Cost: 0.046882264\nEpoch: 807, Cost: 0.047036514\nEpoch: 808, Cost: 0.047421496\nEpoch: 809, Cost: 0.047675893\nEpoch: 810, Cost: 0.047734085\nEpoch: 811, Cost: 0.047855217\nEpoch: 812, Cost: 0.048057146\nEpoch: 813, Cost: 0.048330884\nEpoch: 814, Cost: 0.048036154\nEpoch: 815, Cost: 0.047724605\nEpoch: 816, Cost: 0.047470961\nEpoch: 817, Cost: 0.047360688\nEpoch: 818, Cost: 0.047077116\nEpoch: 819, Cost: 0.046532251\nEpoch: 820, Cost: 0.045984402\nEpoch: 821, Cost: 0.045982514\nEpoch: 822, Cost: 0.046345711\nEpoch: 823, Cost: 0.046898920\nEpoch: 824, Cost: 0.046996325\nEpoch: 825, Cost: 0.047190461\nEpoch: 826, Cost: 0.047233474\nEpoch: 827, Cost: 0.047676560\nEpoch: 828, Cost: 0.047697864\nEpoch: 829, Cost: 0.047763668\nEpoch: 830, Cost: 0.047581740\nEpoch: 831, Cost: 0.047570299\nEpoch: 832, Cost: 0.047175776\nEpoch: 833, Cost: 0.046443623\nEpoch: 834, Cost: 0.045937456\nEpoch: 835, Cost: 0.045865621\nEpoch: 836, Cost: 0.045681845\nEpoch: 837, Cost: 0.045338009\nEpoch: 838, Cost: 0.045207012\nEpoch: 839, Cost: 0.045742761\nEpoch: 840, Cost: 0.045915768\nEpoch: 841, Cost: 0.045627393\nEpoch: 842, Cost: 0.045049254\nEpoch: 843, Cost: 0.045210987\nEpoch: 844, Cost: 0.045638081\nEpoch: 845, Cost: 0.045678064\nEpoch: 846, Cost: 0.045315463\nEpoch: 847, Cost: 0.045047510\nEpoch: 848, Cost: 0.044998161\nEpoch: 849, Cost: 0.044950426\nEpoch: 850, Cost: 0.044775359\nEpoch: 851, Cost: 0.044650067\nEpoch: 852, Cost: 0.044775475\nEpoch: 853, Cost: 0.044943344\nEpoch: 854, Cost: 0.045295190\nEpoch: 855, Cost: 0.045459997\nEpoch: 856, Cost: 0.045647401\nEpoch: 857, Cost: 0.045857202\nEpoch: 858, Cost: 0.046131998\nEpoch: 859, Cost: 0.046817392\nEpoch: 860, Cost: 0.046825018\nEpoch: 861, Cost: 0.046807796\nEpoch: 862, Cost: 0.045836240\nEpoch: 863, Cost: 0.045425534\nEpoch: 864, Cost: 0.045229059\nEpoch: 865, Cost: 0.045394335\nEpoch: 866, Cost: 0.045423068\nEpoch: 867, Cost: 0.045040533\nEpoch: 868, Cost: 0.045168705\nEpoch: 869, Cost: 0.045455046\nEpoch: 870, Cost: 0.046138369\nEpoch: 871, Cost: 0.045976184\nEpoch: 872, Cost: 0.046164218\nEpoch: 873, Cost: 0.046736829\nEpoch: 874, Cost: 0.047907300\nEpoch: 875, Cost: 0.047964543\nEpoch: 876, Cost: 0.046985194\nEpoch: 877, Cost: 0.045503177\nEpoch: 878, Cost: 0.044820972\nEpoch: 879, Cost: 0.044577818\nEpoch: 880, Cost: 0.044318099\nEpoch: 881, Cost: 0.044410523\nEpoch: 882, Cost: 0.044119213\nEpoch: 883, Cost: 0.043741841\nEpoch: 884, Cost: 0.043156117\nEpoch: 885, Cost: 0.043199532\nEpoch: 886, Cost: 0.043350041\nEpoch: 887, Cost: 0.043016903\nEpoch: 888, Cost: 0.042646896\nEpoch: 889, Cost: 0.042644974\nEpoch: 890, Cost: 0.042843089\nEpoch: 891, Cost: 0.042901989\nEpoch: 892, Cost: 0.042869836\nEpoch: 893, Cost: 0.043356001\nEpoch: 894, Cost: 0.043994367\nEpoch: 895, Cost: 0.044850260\nEpoch: 896, Cost: 0.045458034\nEpoch: 897, Cost: 0.046095252\nEpoch: 898, Cost: 0.045742624\nEpoch: 899, Cost: 0.044546857\nEpoch: 900, Cost: 0.042940918\nEpoch: 901, Cost: 0.042141207\nEpoch: 902, Cost: 0.042480778\nEpoch: 903, Cost: 0.043005608\nEpoch: 904, Cost: 0.043088667\nEpoch: 905, Cost: 0.042343523\nEpoch: 906, Cost: 0.041595384\nEpoch: 907, Cost: 0.041376341\nEpoch: 908, Cost: 0.041614622\nEpoch: 909, Cost: 0.041766219\nEpoch: 910, Cost: 0.041510504\nEpoch: 911, Cost: 0.041153084\nEpoch: 912, Cost: 0.041022323\nEpoch: 913, Cost: 0.041162308\nEpoch: 914, Cost: 0.041281980\nEpoch: 915, Cost: 0.041232675\nEpoch: 916, Cost: 0.041152127\nEpoch: 917, Cost: 0.041261077\nEpoch: 918, Cost: 0.041584145\nEpoch: 919, Cost: 0.042032305\nEpoch: 920, Cost: 0.042468883\nEpoch: 921, Cost: 0.043140288\nEpoch: 922, Cost: 0.043835644\nEpoch: 923, Cost: 0.044587370\nEpoch: 924, Cost: 0.044522837\nEpoch: 925, Cost: 0.043632761\nEpoch: 926, Cost: 0.042648360\nEpoch: 927, Cost: 0.042314220\nEpoch: 928, Cost: 0.043579068\nEpoch: 929, Cost: 0.044844557\nEpoch: 930, Cost: 0.046187378\nEpoch: 931, Cost: 0.045180004\nEpoch: 932, Cost: 0.044359643\nEpoch: 933, Cost: 0.044346243\nEpoch: 934, Cost: 0.045430314\nEpoch: 935, Cost: 0.045958977\nEpoch: 936, Cost: 0.044703986\nEpoch: 937, Cost: 0.043513540\nEpoch: 938, Cost: 0.043888297\nEpoch: 939, Cost: 0.044219155\nEpoch: 940, Cost: 0.043651469\nEpoch: 941, Cost: 0.042910974\nEpoch: 942, Cost: 0.042510360\nEpoch: 943, Cost: 0.041946433\nEpoch: 944, Cost: 0.040875096\nEpoch: 945, Cost: 0.040856820\nEpoch: 946, Cost: 0.041509673\nEpoch: 947, Cost: 0.041117560\nEpoch: 948, Cost: 0.040416516\nEpoch: 949, Cost: 0.040507689\nEpoch: 950, Cost: 0.040727247\nEpoch: 951, Cost: 0.040336393\nEpoch: 952, Cost: 0.039897099\nEpoch: 953, Cost: 0.040165082\nEpoch: 954, Cost: 0.040374964\nEpoch: 955, Cost: 0.039978873\nEpoch: 956, Cost: 0.039879888\nEpoch: 957, Cost: 0.040231355\nEpoch: 958, Cost: 0.040382273\nEpoch: 959, Cost: 0.040199380\nEpoch: 960, Cost: 0.040275104\nEpoch: 961, Cost: 0.040743019\nEpoch: 962, Cost: 0.041162584\nEpoch: 963, Cost: 0.041343853\nEpoch: 964, Cost: 0.041388284\nEpoch: 965, Cost: 0.041347090\nEpoch: 966, Cost: 0.040650692\nEpoch: 967, Cost: 0.039962251\nEpoch: 968, Cost: 0.039617956\nEpoch: 969, Cost: 0.039869331\nEpoch: 970, Cost: 0.040488809\nEpoch: 971, Cost: 0.041303851\nEpoch: 972, Cost: 0.042281400\nEpoch: 973, Cost: 0.043152779\nEpoch: 974, Cost: 0.043859906\nEpoch: 975, Cost: 0.044274222\nEpoch: 976, Cost: 0.043999892\nEpoch: 977, Cost: 0.043121282\nEpoch: 978, Cost: 0.042010766\nEpoch: 979, Cost: 0.041331135\nEpoch: 980, Cost: 0.041341349\nEpoch: 981, Cost: 0.041132510\nEpoch: 982, Cost: 0.040648390\nEpoch: 983, Cost: 0.040346008\nEpoch: 984, Cost: 0.040459719\nEpoch: 985, Cost: 0.040763926\nEpoch: 986, Cost: 0.040838502\nEpoch: 987, Cost: 0.041281495\nEpoch: 988, Cost: 0.041787498\nEpoch: 989, Cost: 0.042386767\nEpoch: 990, Cost: 0.042686973\nEpoch: 991, Cost: 0.042872816\nEpoch: 992, Cost: 0.043115810\nEpoch: 993, Cost: 0.042696901\nEpoch: 994, Cost: 0.042213589\nEpoch: 995, Cost: 0.042017423\nEpoch: 996, Cost: 0.042477366\nEpoch: 997, Cost: 0.042827357\nEpoch: 998, Cost: 0.042436730\nEpoch: 999, Cost: 0.042176574\nEpoch: 1000, Cost: 0.042257242\n\u001b[1m 1/32\u001b[0m \u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 637ms/step - loss: 0.0420","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1719737362.941008     118 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1719737362.954956     118 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0429  \nTotal loss:  0.04339296743273735\n\u001b[1m 1/32\u001b[0m \u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 242ms/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1719737363.529856     118 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport sklearn.preprocessing as prep\nimport csv\n\nclass Autoencoder(tf.keras.Model):\n    def __init__(self, n_input, n_hidden1, n_hidden2, n_hidden3, transfer_function=tf.nn.softplus,\n                 optimizer=tf.keras.optimizers.Adam()):\n        super(Autoencoder, self).__init__()\n        self.n_input = n_input\n        self.n_hidden1 = n_hidden1\n        self.n_hidden2 = n_hidden2\n        self.n_hidden3 = n_hidden3\n        self.transfer = transfer_function\n        \n        # Ensure n_hidden2 is smaller than n_input\n        assert n_hidden2 < n_input, \"n_hidden2 must be smaller than n_input\"\n\n        # Encoder layers\n        self.encoder = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.n_hidden1, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_hidden2, activation=self.transfer)\n        ])\n\n        # Decoder layers\n        self.decoder = tf.keras.Sequential([\n            tf.keras.layers.Dense(self.n_hidden1, activation=self.transfer),\n            tf.keras.layers.Dense(self.n_input, activation='linear')\n        ])\n\n        # Optimizer\n        self.optimizer = optimizer\n\n    def call(self, inputs):\n        encoded = self.encoder(inputs)\n        decoded = self.decoder(encoded)\n        return decoded\n\n# Helper function to standard scale data\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n# Paths and data loading (adjust paths accordingly)\npath1 = '/kaggle/input/ace-dataset/features/'\nAAC = pd.read_csv(path1 + 'ACE_AAC.csv').iloc[:, 1:]\nASDC = pd.read_csv(path1 + 'ACE_ASDC.csv').iloc[:, 1:]\nOPF_7bit_type_1 = pd.read_csv(path1 + 'opf_7bit_type_1_features.csv').iloc[1:, 1:]\nOPF_7bit_type_2 = pd.read_csv(path1 + 'opf_7bit_type_2_features.csv').iloc[1:, 1:]\nOPF_7bit_type_3 = pd.read_csv(path1 + 'opf_7bit_type_3_features.csv').iloc[1:, 1:]\nOPF_10bit = pd.read_csv(path1 + 'opf_10bit_features.csv').iloc[1:, 1:]\nesmv1 = pd.read_csv(path1 + 'esmv1_feat_ACE.csv').iloc[:, :]\nesm2 = pd.read_csv(path1 + 'esm2_t6_8M_feat_ACE.csv').iloc[:, :]\nprot_t5 = pd.read_csv(path1 + 'ACE_embeddings_prot_t5_xl_bfd.csv').iloc[1:, 1:]\n\n# Concatenate all features into a single array\nall_feat = np.column_stack((AAC, ASDC, OPF_7bit_type_1, OPF_7bit_type_2, OPF_7bit_type_3, OPF_10bit, esmv1, esm2, prot_t5))\n\n# Standard scale the data\nX_train, _ = standard_scale(all_feat, all_feat)\n\n# Define parameters\nnum_features = X_train.shape[1]\nn_samples, _ = np.shape(X_train)\ntraining_epochs = 1000\nbatch_size = X_train.shape[0] - 1\ndisplay_step = 1\n\n# Initialize and train autoencoder\nautoencoder = Autoencoder(\n    n_input=num_features,\n    n_hidden1=800,\n    n_hidden2=200,  # Bottleneck layer with lower dimension\n    n_hidden3=800,\n    transfer_function=tf.nn.softplus,\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n\n# Convert X_train to TensorFlow tensor\nX_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    \n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = X_train_tf[i * batch_size:(i + 1) * batch_size]\n        \n        # Fit training using batch data\n        with tf.GradientTape() as tape:\n            reconstruction = autoencoder(batch_xs)\n            loss = tf.reduce_mean(tf.square(batch_xs - reconstruction))\n        \n        gradients = tape.gradient(loss, autoencoder.trainable_variables)\n        autoencoder.optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n        \n        # Compute average loss\n        avg_cost += loss / n_samples * batch_size\n    \n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print(\"Epoch:\", '%d,' % (epoch + 1),\n              \"Cost:\", \"{:.9f}\".format(avg_cost.numpy()))\n\n# Compile the model\nautoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n\n# Evaluate the loss on the training set\nloss = autoencoder.evaluate(X_train_tf, X_train_tf)\nprint(\"Total loss: \", loss)\n\n# Transform and reconstruct the data\nX_test_reconstruct = autoencoder.predict(X_train_tf)\n\n# Save reconstructed data to CSV\nwith open('ting_auto.csv', 'w', newline='') as fout:\n    writer = csv.writer(fout, delimiter=',')\n    for i in X_test_reconstruct:\n        writer.writerow(i)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:56:19.534609Z","iopub.execute_input":"2024-06-30T08:56:19.535407Z","iopub.status.idle":"2024-06-30T08:57:06.349306Z","shell.execute_reply.started":"2024-06-30T08:56:19.535372Z","shell.execute_reply":"2024-06-30T08:57:06.348417Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch: 1, Cost: 1.217963576\nEpoch: 2, Cost: 1.037773252\nEpoch: 3, Cost: 1.017764211\nEpoch: 4, Cost: 1.011155009\nEpoch: 5, Cost: 0.993533134\nEpoch: 6, Cost: 0.967011273\nEpoch: 7, Cost: 0.941068053\nEpoch: 8, Cost: 0.918215811\nEpoch: 9, Cost: 0.896862626\nEpoch: 10, Cost: 0.876513362\nEpoch: 11, Cost: 0.856943667\nEpoch: 12, Cost: 0.837242305\nEpoch: 13, Cost: 0.817873716\nEpoch: 14, Cost: 0.799437642\nEpoch: 15, Cost: 0.781643212\nEpoch: 16, Cost: 0.763967633\nEpoch: 17, Cost: 0.746426642\nEpoch: 18, Cost: 0.729302764\nEpoch: 19, Cost: 0.712934613\nEpoch: 20, Cost: 0.696876347\nEpoch: 21, Cost: 0.681015790\nEpoch: 22, Cost: 0.665298462\nEpoch: 23, Cost: 0.649683475\nEpoch: 24, Cost: 0.634131491\nEpoch: 25, Cost: 0.618766427\nEpoch: 26, Cost: 0.603630602\nEpoch: 27, Cost: 0.588757515\nEpoch: 28, Cost: 0.574198186\nEpoch: 29, Cost: 0.559990585\nEpoch: 30, Cost: 0.546244025\nEpoch: 31, Cost: 0.533082604\nEpoch: 32, Cost: 0.520389259\nEpoch: 33, Cost: 0.508247852\nEpoch: 34, Cost: 0.496640861\nEpoch: 35, Cost: 0.485556722\nEpoch: 36, Cost: 0.475007057\nEpoch: 37, Cost: 0.465052843\nEpoch: 38, Cost: 0.455591589\nEpoch: 39, Cost: 0.446577787\nEpoch: 40, Cost: 0.438009650\nEpoch: 41, Cost: 0.429991961\nEpoch: 42, Cost: 0.422713459\nEpoch: 43, Cost: 0.416124046\nEpoch: 44, Cost: 0.409189165\nEpoch: 45, Cost: 0.401293755\nEpoch: 46, Cost: 0.394840330\nEpoch: 47, Cost: 0.390508682\nEpoch: 48, Cost: 0.384327829\nEpoch: 49, Cost: 0.376127630\nEpoch: 50, Cost: 0.371315837\nEpoch: 51, Cost: 0.366156906\nEpoch: 52, Cost: 0.359292358\nEpoch: 53, Cost: 0.355198026\nEpoch: 54, Cost: 0.350170285\nEpoch: 55, Cost: 0.343853593\nEpoch: 56, Cost: 0.339930058\nEpoch: 57, Cost: 0.335461766\nEpoch: 58, Cost: 0.330109179\nEpoch: 59, Cost: 0.326409042\nEpoch: 60, Cost: 0.322157919\nEpoch: 61, Cost: 0.317174673\nEpoch: 62, Cost: 0.313859344\nEpoch: 63, Cost: 0.310572594\nEpoch: 64, Cost: 0.307030797\nEpoch: 65, Cost: 0.304746896\nEpoch: 66, Cost: 0.301082015\nEpoch: 67, Cost: 0.295198619\nEpoch: 68, Cost: 0.291796207\nEpoch: 69, Cost: 0.289635688\nEpoch: 70, Cost: 0.285737813\nEpoch: 71, Cost: 0.282053679\nEpoch: 72, Cost: 0.279066235\nEpoch: 73, Cost: 0.276040018\nEpoch: 74, Cost: 0.273230612\nEpoch: 75, Cost: 0.269948006\nEpoch: 76, Cost: 0.266876101\nEpoch: 77, Cost: 0.264614135\nEpoch: 78, Cost: 0.261875898\nEpoch: 79, Cost: 0.258904397\nEpoch: 80, Cost: 0.256853908\nEpoch: 81, Cost: 0.255655110\nEpoch: 82, Cost: 0.255418241\nEpoch: 83, Cost: 0.255353898\nEpoch: 84, Cost: 0.251577914\nEpoch: 85, Cost: 0.245188653\nEpoch: 86, Cost: 0.244063273\nEpoch: 87, Cost: 0.243195146\nEpoch: 88, Cost: 0.238600835\nEpoch: 89, Cost: 0.236752033\nEpoch: 90, Cost: 0.235434830\nEpoch: 91, Cost: 0.232083783\nEpoch: 92, Cost: 0.230315536\nEpoch: 93, Cost: 0.228327736\nEpoch: 94, Cost: 0.226002276\nEpoch: 95, Cost: 0.223838612\nEpoch: 96, Cost: 0.222130015\nEpoch: 97, Cost: 0.220216393\nEpoch: 98, Cost: 0.217865258\nEpoch: 99, Cost: 0.216335848\nEpoch: 100, Cost: 0.214453205\nEpoch: 101, Cost: 0.212220177\nEpoch: 102, Cost: 0.210816219\nEpoch: 103, Cost: 0.209005415\nEpoch: 104, Cost: 0.206940576\nEpoch: 105, Cost: 0.205518439\nEpoch: 106, Cost: 0.203734815\nEpoch: 107, Cost: 0.201991320\nEpoch: 108, Cost: 0.200511336\nEpoch: 109, Cost: 0.198919296\nEpoch: 110, Cost: 0.197663099\nEpoch: 111, Cost: 0.196871385\nEpoch: 112, Cost: 0.196995094\nEpoch: 113, Cost: 0.198600307\nEpoch: 114, Cost: 0.198858485\nEpoch: 115, Cost: 0.193661496\nEpoch: 116, Cost: 0.188754112\nEpoch: 117, Cost: 0.190288410\nEpoch: 118, Cost: 0.189270183\nEpoch: 119, Cost: 0.184674531\nEpoch: 120, Cost: 0.184889838\nEpoch: 121, Cost: 0.183781803\nEpoch: 122, Cost: 0.180562705\nEpoch: 123, Cost: 0.180653006\nEpoch: 124, Cost: 0.178898811\nEpoch: 125, Cost: 0.176811442\nEpoch: 126, Cost: 0.176449284\nEpoch: 127, Cost: 0.174369857\nEpoch: 128, Cost: 0.173240304\nEpoch: 129, Cost: 0.172348708\nEpoch: 130, Cost: 0.170434356\nEpoch: 131, Cost: 0.169723317\nEpoch: 132, Cost: 0.168467894\nEpoch: 133, Cost: 0.167029724\nEpoch: 134, Cost: 0.166570410\nEpoch: 135, Cost: 0.165578276\nEpoch: 136, Cost: 0.165092349\nEpoch: 137, Cost: 0.165383592\nEpoch: 138, Cost: 0.164712042\nEpoch: 139, Cost: 0.163291335\nEpoch: 140, Cost: 0.160594240\nEpoch: 141, Cost: 0.158070207\nEpoch: 142, Cost: 0.157803103\nEpoch: 143, Cost: 0.157594442\nEpoch: 144, Cost: 0.156133085\nEpoch: 145, Cost: 0.154292539\nEpoch: 146, Cost: 0.153206661\nEpoch: 147, Cost: 0.153144479\nEpoch: 148, Cost: 0.152492926\nEpoch: 149, Cost: 0.151206061\nEpoch: 150, Cost: 0.151341543\nEpoch: 151, Cost: 0.152570367\nEpoch: 152, Cost: 0.153259858\nEpoch: 153, Cost: 0.151206002\nEpoch: 154, Cost: 0.147581533\nEpoch: 155, Cost: 0.145636797\nEpoch: 156, Cost: 0.145590857\nEpoch: 157, Cost: 0.144948646\nEpoch: 158, Cost: 0.143390402\nEpoch: 159, Cost: 0.141997784\nEpoch: 160, Cost: 0.141140625\nEpoch: 161, Cost: 0.140606448\nEpoch: 162, Cost: 0.139596075\nEpoch: 163, Cost: 0.138077483\nEpoch: 164, Cost: 0.137270689\nEpoch: 165, Cost: 0.136968970\nEpoch: 166, Cost: 0.135868862\nEpoch: 167, Cost: 0.134498894\nEpoch: 168, Cost: 0.133951336\nEpoch: 169, Cost: 0.133506164\nEpoch: 170, Cost: 0.132371306\nEpoch: 171, Cost: 0.131287828\nEpoch: 172, Cost: 0.130825013\nEpoch: 173, Cost: 0.130335256\nEpoch: 174, Cost: 0.129475817\nEpoch: 175, Cost: 0.128985107\nEpoch: 176, Cost: 0.129315212\nEpoch: 177, Cost: 0.130243540\nEpoch: 178, Cost: 0.131262809\nEpoch: 179, Cost: 0.131707653\nEpoch: 180, Cost: 0.128997713\nEpoch: 181, Cost: 0.124844477\nEpoch: 182, Cost: 0.124496482\nEpoch: 183, Cost: 0.125860497\nEpoch: 184, Cost: 0.123913772\nEpoch: 185, Cost: 0.121489801\nEpoch: 186, Cost: 0.122153915\nEpoch: 187, Cost: 0.121716328\nEpoch: 188, Cost: 0.119459361\nEpoch: 189, Cost: 0.119272009\nEpoch: 190, Cost: 0.119260214\nEpoch: 191, Cost: 0.117531955\nEpoch: 192, Cost: 0.116959237\nEpoch: 193, Cost: 0.117026269\nEpoch: 194, Cost: 0.115780689\nEpoch: 195, Cost: 0.114997603\nEpoch: 196, Cost: 0.114955783\nEpoch: 197, Cost: 0.114157729\nEpoch: 198, Cost: 0.113623612\nEpoch: 199, Cost: 0.113901906\nEpoch: 200, Cost: 0.113951974\nEpoch: 201, Cost: 0.114222631\nEpoch: 202, Cost: 0.115182012\nEpoch: 203, Cost: 0.114896253\nEpoch: 204, Cost: 0.112935491\nEpoch: 205, Cost: 0.111254044\nEpoch: 206, Cost: 0.111979336\nEpoch: 207, Cost: 0.114863448\nEpoch: 208, Cost: 0.115357533\nEpoch: 209, Cost: 0.110682681\nEpoch: 210, Cost: 0.107062981\nEpoch: 211, Cost: 0.108809255\nEpoch: 212, Cost: 0.108426817\nEpoch: 213, Cost: 0.105773009\nEpoch: 214, Cost: 0.106117569\nEpoch: 215, Cost: 0.105634518\nEpoch: 216, Cost: 0.103638612\nEpoch: 217, Cost: 0.103984229\nEpoch: 218, Cost: 0.103700459\nEpoch: 219, Cost: 0.101841584\nEpoch: 220, Cost: 0.101647347\nEpoch: 221, Cost: 0.101654261\nEpoch: 222, Cost: 0.100548655\nEpoch: 223, Cost: 0.099875398\nEpoch: 224, Cost: 0.099470638\nEpoch: 225, Cost: 0.098897703\nEpoch: 226, Cost: 0.098415114\nEpoch: 227, Cost: 0.097786389\nEpoch: 228, Cost: 0.097163923\nEpoch: 229, Cost: 0.096726820\nEpoch: 230, Cost: 0.096275806\nEpoch: 231, Cost: 0.095769241\nEpoch: 232, Cost: 0.095193997\nEpoch: 233, Cost: 0.094657354\nEpoch: 234, Cost: 0.094271690\nEpoch: 235, Cost: 0.093849570\nEpoch: 236, Cost: 0.093367808\nEpoch: 237, Cost: 0.093026325\nEpoch: 238, Cost: 0.092847109\nEpoch: 239, Cost: 0.093006186\nEpoch: 240, Cost: 0.093971796\nEpoch: 241, Cost: 0.096208297\nEpoch: 242, Cost: 0.099062331\nEpoch: 243, Cost: 0.099943466\nEpoch: 244, Cost: 0.095345080\nEpoch: 245, Cost: 0.092121825\nEpoch: 246, Cost: 0.094449975\nEpoch: 247, Cost: 0.093315236\nEpoch: 248, Cost: 0.088893056\nEpoch: 249, Cost: 0.090430260\nEpoch: 250, Cost: 0.090849787\nEpoch: 251, Cost: 0.087798089\nEpoch: 252, Cost: 0.088511162\nEpoch: 253, Cost: 0.087950148\nEpoch: 254, Cost: 0.086239241\nEpoch: 255, Cost: 0.087250821\nEpoch: 256, Cost: 0.086080760\nEpoch: 257, Cost: 0.085099101\nEpoch: 258, Cost: 0.085485436\nEpoch: 259, Cost: 0.084244475\nEpoch: 260, Cost: 0.084140092\nEpoch: 261, Cost: 0.083966881\nEpoch: 262, Cost: 0.082886815\nEpoch: 263, Cost: 0.082904175\nEpoch: 264, Cost: 0.082341820\nEpoch: 265, Cost: 0.081755944\nEpoch: 266, Cost: 0.081751525\nEpoch: 267, Cost: 0.081059970\nEpoch: 268, Cost: 0.080661409\nEpoch: 269, Cost: 0.080470376\nEpoch: 270, Cost: 0.079907045\nEpoch: 271, Cost: 0.079790339\nEpoch: 272, Cost: 0.079762973\nEpoch: 273, Cost: 0.079776116\nEpoch: 274, Cost: 0.080683276\nEpoch: 275, Cost: 0.082536072\nEpoch: 276, Cost: 0.085468747\nEpoch: 277, Cost: 0.087687038\nEpoch: 278, Cost: 0.085015155\nEpoch: 279, Cost: 0.079320177\nEpoch: 280, Cost: 0.079200320\nEpoch: 281, Cost: 0.082212657\nEpoch: 282, Cost: 0.080077581\nEpoch: 283, Cost: 0.076687865\nEpoch: 284, Cost: 0.078414291\nEpoch: 285, Cost: 0.077892937\nEpoch: 286, Cost: 0.074978717\nEpoch: 287, Cost: 0.076319627\nEpoch: 288, Cost: 0.076395132\nEpoch: 289, Cost: 0.074255437\nEpoch: 290, Cost: 0.075035952\nEpoch: 291, Cost: 0.074652791\nEpoch: 292, Cost: 0.073066927\nEpoch: 293, Cost: 0.073563524\nEpoch: 294, Cost: 0.072985031\nEpoch: 295, Cost: 0.072090581\nEpoch: 296, Cost: 0.072517231\nEpoch: 297, Cost: 0.071917087\nEpoch: 298, Cost: 0.071287669\nEpoch: 299, Cost: 0.071402945\nEpoch: 300, Cost: 0.070815429\nEpoch: 301, Cost: 0.070342667\nEpoch: 302, Cost: 0.070294142\nEpoch: 303, Cost: 0.069783621\nEpoch: 304, Cost: 0.069390014\nEpoch: 305, Cost: 0.069245420\nEpoch: 306, Cost: 0.068845689\nEpoch: 307, Cost: 0.068538494\nEpoch: 308, Cost: 0.068410404\nEpoch: 309, Cost: 0.068153448\nEpoch: 310, Cost: 0.068012610\nEpoch: 311, Cost: 0.068088919\nEpoch: 312, Cost: 0.068270944\nEpoch: 313, Cost: 0.068708099\nEpoch: 314, Cost: 0.069510676\nEpoch: 315, Cost: 0.070109732\nEpoch: 316, Cost: 0.070002913\nEpoch: 317, Cost: 0.068581045\nEpoch: 318, Cost: 0.066671722\nEpoch: 319, Cost: 0.065837853\nEpoch: 320, Cost: 0.066452451\nEpoch: 321, Cost: 0.066999644\nEpoch: 322, Cost: 0.066199213\nEpoch: 323, Cost: 0.064981893\nEpoch: 324, Cost: 0.064924553\nEpoch: 325, Cost: 0.065846518\nEpoch: 326, Cost: 0.066438094\nEpoch: 327, Cost: 0.066616103\nEpoch: 328, Cost: 0.067404635\nEpoch: 329, Cost: 0.068659179\nEpoch: 330, Cost: 0.068289086\nEpoch: 331, Cost: 0.065823376\nEpoch: 332, Cost: 0.063938662\nEpoch: 333, Cost: 0.064401515\nEpoch: 334, Cost: 0.065121621\nEpoch: 335, Cost: 0.064205125\nEpoch: 336, Cost: 0.062773392\nEpoch: 337, Cost: 0.062134366\nEpoch: 338, Cost: 0.062107731\nEpoch: 339, Cost: 0.062103149\nEpoch: 340, Cost: 0.061746392\nEpoch: 341, Cost: 0.060953636\nEpoch: 342, Cost: 0.060451567\nEpoch: 343, Cost: 0.060630821\nEpoch: 344, Cost: 0.060602453\nEpoch: 345, Cost: 0.059788242\nEpoch: 346, Cost: 0.059100028\nEpoch: 347, Cost: 0.059207607\nEpoch: 348, Cost: 0.059271090\nEpoch: 349, Cost: 0.058705900\nEpoch: 350, Cost: 0.058199123\nEpoch: 351, Cost: 0.058217198\nEpoch: 352, Cost: 0.058227036\nEpoch: 353, Cost: 0.057845246\nEpoch: 354, Cost: 0.057429872\nEpoch: 355, Cost: 0.057286546\nEpoch: 356, Cost: 0.057196733\nEpoch: 357, Cost: 0.056967299\nEpoch: 358, Cost: 0.056751046\nEpoch: 359, Cost: 0.056704618\nEpoch: 360, Cost: 0.056800857\nEpoch: 361, Cost: 0.057129584\nEpoch: 362, Cost: 0.057938926\nEpoch: 363, Cost: 0.059564624\nEpoch: 364, Cost: 0.061700951\nEpoch: 365, Cost: 0.063247554\nEpoch: 366, Cost: 0.061849538\nEpoch: 367, Cost: 0.058576748\nEpoch: 368, Cost: 0.058085740\nEpoch: 369, Cost: 0.060415499\nEpoch: 370, Cost: 0.059871111\nEpoch: 371, Cost: 0.056004237\nEpoch: 372, Cost: 0.055456880\nEpoch: 373, Cost: 0.056947295\nEpoch: 374, Cost: 0.055867810\nEpoch: 375, Cost: 0.055144209\nEpoch: 376, Cost: 0.055545542\nEpoch: 377, Cost: 0.054209612\nEpoch: 378, Cost: 0.053336874\nEpoch: 379, Cost: 0.054161195\nEpoch: 380, Cost: 0.053752780\nEpoch: 381, Cost: 0.052910488\nEpoch: 382, Cost: 0.053126093\nEpoch: 383, Cost: 0.052720398\nEpoch: 384, Cost: 0.052147679\nEpoch: 385, Cost: 0.052492771\nEpoch: 386, Cost: 0.052292522\nEpoch: 387, Cost: 0.051549006\nEpoch: 388, Cost: 0.051505145\nEpoch: 389, Cost: 0.051512904\nEpoch: 390, Cost: 0.051142722\nEpoch: 391, Cost: 0.051073652\nEpoch: 392, Cost: 0.051077746\nEpoch: 393, Cost: 0.050842017\nEpoch: 394, Cost: 0.050865043\nEpoch: 395, Cost: 0.051188599\nEpoch: 396, Cost: 0.051555526\nEpoch: 397, Cost: 0.052283239\nEpoch: 398, Cost: 0.053401474\nEpoch: 399, Cost: 0.054353729\nEpoch: 400, Cost: 0.054325566\nEpoch: 401, Cost: 0.053163003\nEpoch: 402, Cost: 0.051413901\nEpoch: 403, Cost: 0.051036593\nEpoch: 404, Cost: 0.052132696\nEpoch: 405, Cost: 0.052475363\nEpoch: 406, Cost: 0.050563972\nEpoch: 407, Cost: 0.048699725\nEpoch: 408, Cost: 0.048934616\nEpoch: 409, Cost: 0.049721476\nEpoch: 410, Cost: 0.049159851\nEpoch: 411, Cost: 0.048197854\nEpoch: 412, Cost: 0.048284028\nEpoch: 413, Cost: 0.048500348\nEpoch: 414, Cost: 0.047797490\nEpoch: 415, Cost: 0.047118843\nEpoch: 416, Cost: 0.047342628\nEpoch: 417, Cost: 0.047571968\nEpoch: 418, Cost: 0.047037534\nEpoch: 419, Cost: 0.046508204\nEpoch: 420, Cost: 0.046573400\nEpoch: 421, Cost: 0.046651486\nEpoch: 422, Cost: 0.046335481\nEpoch: 423, Cost: 0.046110503\nEpoch: 424, Cost: 0.046287000\nEpoch: 425, Cost: 0.046564773\nEpoch: 426, Cost: 0.046799503\nEpoch: 427, Cost: 0.047496941\nEpoch: 428, Cost: 0.048882674\nEpoch: 429, Cost: 0.050670609\nEpoch: 430, Cost: 0.051307298\nEpoch: 431, Cost: 0.050005451\nEpoch: 432, Cost: 0.047061622\nEpoch: 433, Cost: 0.045607120\nEpoch: 434, Cost: 0.046704009\nEpoch: 435, Cost: 0.047943115\nEpoch: 436, Cost: 0.047256559\nEpoch: 437, Cost: 0.046019830\nEpoch: 438, Cost: 0.046525795\nEpoch: 439, Cost: 0.047422748\nEpoch: 440, Cost: 0.046783183\nEpoch: 441, Cost: 0.045916967\nEpoch: 442, Cost: 0.046142634\nEpoch: 443, Cost: 0.045959786\nEpoch: 444, Cost: 0.045041084\nEpoch: 445, Cost: 0.044560656\nEpoch: 446, Cost: 0.044456068\nEpoch: 447, Cost: 0.043984264\nEpoch: 448, Cost: 0.043679912\nEpoch: 449, Cost: 0.043579329\nEpoch: 450, Cost: 0.043110993\nEpoch: 451, Cost: 0.042671349\nEpoch: 452, Cost: 0.042743638\nEpoch: 453, Cost: 0.042769752\nEpoch: 454, Cost: 0.042491477\nEpoch: 455, Cost: 0.042363539\nEpoch: 456, Cost: 0.042274553\nEpoch: 457, Cost: 0.041894671\nEpoch: 458, Cost: 0.041547202\nEpoch: 459, Cost: 0.041541114\nEpoch: 460, Cost: 0.041582238\nEpoch: 461, Cost: 0.041436926\nEpoch: 462, Cost: 0.041277267\nEpoch: 463, Cost: 0.041185338\nEpoch: 464, Cost: 0.041077770\nEpoch: 465, Cost: 0.041000918\nEpoch: 466, Cost: 0.041164886\nEpoch: 467, Cost: 0.041721348\nEpoch: 468, Cost: 0.042861588\nEpoch: 469, Cost: 0.044888441\nEpoch: 470, Cost: 0.047603093\nEpoch: 471, Cost: 0.049269818\nEpoch: 472, Cost: 0.046915885\nEpoch: 473, Cost: 0.042223427\nEpoch: 474, Cost: 0.041648313\nEpoch: 475, Cost: 0.044424433\nEpoch: 476, Cost: 0.043696202\nEpoch: 477, Cost: 0.040504463\nEpoch: 478, Cost: 0.041059185\nEpoch: 479, Cost: 0.042124394\nEpoch: 480, Cost: 0.040161416\nEpoch: 481, Cost: 0.039937440\nEpoch: 482, Cost: 0.041044965\nEpoch: 483, Cost: 0.039832715\nEpoch: 484, Cost: 0.039417662\nEpoch: 485, Cost: 0.040145043\nEpoch: 486, Cost: 0.039187908\nEpoch: 487, Cost: 0.038811937\nEpoch: 488, Cost: 0.039337266\nEpoch: 489, Cost: 0.038742337\nEpoch: 490, Cost: 0.038587339\nEpoch: 491, Cost: 0.038976014\nEpoch: 492, Cost: 0.038567685\nEpoch: 493, Cost: 0.038569938\nEpoch: 494, Cost: 0.038962778\nEpoch: 495, Cost: 0.038882133\nEpoch: 496, Cost: 0.039259676\nEpoch: 497, Cost: 0.039990127\nEpoch: 498, Cost: 0.040379886\nEpoch: 499, Cost: 0.040748946\nEpoch: 500, Cost: 0.040653452\nEpoch: 501, Cost: 0.039433051\nEpoch: 502, Cost: 0.038212433\nEpoch: 503, Cost: 0.037739199\nEpoch: 504, Cost: 0.037954476\nEpoch: 505, Cost: 0.038582802\nEpoch: 506, Cost: 0.038700741\nEpoch: 507, Cost: 0.038088474\nEpoch: 508, Cost: 0.037789673\nEpoch: 509, Cost: 0.038207907\nEpoch: 510, Cost: 0.038819656\nEpoch: 511, Cost: 0.038968500\nEpoch: 512, Cost: 0.038392913\nEpoch: 513, Cost: 0.037580498\nEpoch: 514, Cost: 0.037115112\nEpoch: 515, Cost: 0.036808871\nEpoch: 516, Cost: 0.036391325\nEpoch: 517, Cost: 0.036138989\nEpoch: 518, Cost: 0.036299482\nEpoch: 519, Cost: 0.036606204\nEpoch: 520, Cost: 0.036706898\nEpoch: 521, Cost: 0.036526728\nEpoch: 522, Cost: 0.036402967\nEpoch: 523, Cost: 0.036672149\nEpoch: 524, Cost: 0.037302867\nEpoch: 525, Cost: 0.038140636\nEpoch: 526, Cost: 0.039053682\nEpoch: 527, Cost: 0.039803047\nEpoch: 528, Cost: 0.039676670\nEpoch: 529, Cost: 0.038350765\nEpoch: 530, Cost: 0.036678884\nEpoch: 531, Cost: 0.036072481\nEpoch: 532, Cost: 0.036633935\nEpoch: 533, Cost: 0.037011005\nEpoch: 534, Cost: 0.036273044\nEpoch: 535, Cost: 0.035155471\nEpoch: 536, Cost: 0.034912955\nEpoch: 537, Cost: 0.035300251\nEpoch: 538, Cost: 0.035226021\nEpoch: 539, Cost: 0.034581751\nEpoch: 540, Cost: 0.034223419\nEpoch: 541, Cost: 0.034462627\nEpoch: 542, Cost: 0.034676574\nEpoch: 543, Cost: 0.034413546\nEpoch: 544, Cost: 0.034036573\nEpoch: 545, Cost: 0.033962067\nEpoch: 546, Cost: 0.034048505\nEpoch: 547, Cost: 0.034060534\nEpoch: 548, Cost: 0.034060497\nEpoch: 549, Cost: 0.034211200\nEpoch: 550, Cost: 0.034461092\nEpoch: 551, Cost: 0.034737729\nEpoch: 552, Cost: 0.035038769\nEpoch: 553, Cost: 0.035344668\nEpoch: 554, Cost: 0.035281077\nEpoch: 555, Cost: 0.034676146\nEpoch: 556, Cost: 0.033731196\nEpoch: 557, Cost: 0.033080064\nEpoch: 558, Cost: 0.033020403\nEpoch: 559, Cost: 0.033391532\nEpoch: 560, Cost: 0.033890087\nEpoch: 561, Cost: 0.034406174\nEpoch: 562, Cost: 0.034765400\nEpoch: 563, Cost: 0.035108827\nEpoch: 564, Cost: 0.035234310\nEpoch: 565, Cost: 0.035372216\nEpoch: 566, Cost: 0.035093680\nEpoch: 567, Cost: 0.034463391\nEpoch: 568, Cost: 0.033539828\nEpoch: 569, Cost: 0.033191778\nEpoch: 570, Cost: 0.033628080\nEpoch: 571, Cost: 0.034245737\nEpoch: 572, Cost: 0.034096293\nEpoch: 573, Cost: 0.033268679\nEpoch: 574, Cost: 0.032529071\nEpoch: 575, Cost: 0.032523204\nEpoch: 576, Cost: 0.032742005\nEpoch: 577, Cost: 0.032668788\nEpoch: 578, Cost: 0.032429997\nEpoch: 579, Cost: 0.032463860\nEpoch: 580, Cost: 0.032720704\nEpoch: 581, Cost: 0.032666873\nEpoch: 582, Cost: 0.032258131\nEpoch: 583, Cost: 0.031879049\nEpoch: 584, Cost: 0.031733148\nEpoch: 585, Cost: 0.031653974\nEpoch: 586, Cost: 0.031448346\nEpoch: 587, Cost: 0.031313058\nEpoch: 588, Cost: 0.031377159\nEpoch: 589, Cost: 0.031478088\nEpoch: 590, Cost: 0.031411670\nEpoch: 591, Cost: 0.031198297\nEpoch: 592, Cost: 0.031128457\nEpoch: 593, Cost: 0.031302657\nEpoch: 594, Cost: 0.031625792\nEpoch: 595, Cost: 0.031977504\nEpoch: 596, Cost: 0.032346461\nEpoch: 597, Cost: 0.032763280\nEpoch: 598, Cost: 0.033098016\nEpoch: 599, Cost: 0.033259016\nEpoch: 600, Cost: 0.033132147\nEpoch: 601, Cost: 0.033003170\nEpoch: 602, Cost: 0.032871243\nEpoch: 603, Cost: 0.032817312\nEpoch: 604, Cost: 0.032329772\nEpoch: 605, Cost: 0.031573862\nEpoch: 606, Cost: 0.031061046\nEpoch: 607, Cost: 0.031301975\nEpoch: 608, Cost: 0.032042548\nEpoch: 609, Cost: 0.032094609\nEpoch: 610, Cost: 0.031366765\nEpoch: 611, Cost: 0.030616205\nEpoch: 612, Cost: 0.030627647\nEpoch: 613, Cost: 0.030899541\nEpoch: 614, Cost: 0.030530540\nEpoch: 615, Cost: 0.029948181\nEpoch: 616, Cost: 0.029917559\nEpoch: 617, Cost: 0.030302040\nEpoch: 618, Cost: 0.030277299\nEpoch: 619, Cost: 0.029716397\nEpoch: 620, Cost: 0.029331001\nEpoch: 621, Cost: 0.029334979\nEpoch: 622, Cost: 0.029237883\nEpoch: 623, Cost: 0.028929621\nEpoch: 624, Cost: 0.028791329\nEpoch: 625, Cost: 0.028948268\nEpoch: 626, Cost: 0.029068256\nEpoch: 627, Cost: 0.028946333\nEpoch: 628, Cost: 0.028865144\nEpoch: 629, Cost: 0.029019423\nEpoch: 630, Cost: 0.029305510\nEpoch: 631, Cost: 0.029617488\nEpoch: 632, Cost: 0.030099643\nEpoch: 633, Cost: 0.030786423\nEpoch: 634, Cost: 0.031513333\nEpoch: 635, Cost: 0.031615615\nEpoch: 636, Cost: 0.031099336\nEpoch: 637, Cost: 0.030186975\nEpoch: 638, Cost: 0.029647870\nEpoch: 639, Cost: 0.029925464\nEpoch: 640, Cost: 0.030597571\nEpoch: 641, Cost: 0.030846063\nEpoch: 642, Cost: 0.030246845\nEpoch: 643, Cost: 0.029310303\nEpoch: 644, Cost: 0.029202813\nEpoch: 645, Cost: 0.029713456\nEpoch: 646, Cost: 0.030017035\nEpoch: 647, Cost: 0.029506294\nEpoch: 648, Cost: 0.028908638\nEpoch: 649, Cost: 0.028719205\nEpoch: 650, Cost: 0.028597040\nEpoch: 651, Cost: 0.028151929\nEpoch: 652, Cost: 0.027702043\nEpoch: 653, Cost: 0.027682398\nEpoch: 654, Cost: 0.027806565\nEpoch: 655, Cost: 0.027652385\nEpoch: 656, Cost: 0.027382739\nEpoch: 657, Cost: 0.027371921\nEpoch: 658, Cost: 0.027512498\nEpoch: 659, Cost: 0.027477244\nEpoch: 660, Cost: 0.027253632\nEpoch: 661, Cost: 0.027107652\nEpoch: 662, Cost: 0.027121987\nEpoch: 663, Cost: 0.027157966\nEpoch: 664, Cost: 0.027162760\nEpoch: 665, Cost: 0.027189132\nEpoch: 666, Cost: 0.027260184\nEpoch: 667, Cost: 0.027267734\nEpoch: 668, Cost: 0.027205205\nEpoch: 669, Cost: 0.027124763\nEpoch: 670, Cost: 0.027085062\nEpoch: 671, Cost: 0.027007900\nEpoch: 672, Cost: 0.026886566\nEpoch: 673, Cost: 0.026791565\nEpoch: 674, Cost: 0.026859658\nEpoch: 675, Cost: 0.027147301\nEpoch: 676, Cost: 0.027627505\nEpoch: 677, Cost: 0.028288918\nEpoch: 678, Cost: 0.029029943\nEpoch: 679, Cost: 0.029680934\nEpoch: 680, Cost: 0.029931469\nEpoch: 681, Cost: 0.029513827\nEpoch: 682, Cost: 0.028812265\nEpoch: 683, Cost: 0.028366046\nEpoch: 684, Cost: 0.028475787\nEpoch: 685, Cost: 0.028595215\nEpoch: 686, Cost: 0.028046474\nEpoch: 687, Cost: 0.027197350\nEpoch: 688, Cost: 0.026947336\nEpoch: 689, Cost: 0.027360994\nEpoch: 690, Cost: 0.027370112\nEpoch: 691, Cost: 0.026649265\nEpoch: 692, Cost: 0.026128927\nEpoch: 693, Cost: 0.026383199\nEpoch: 694, Cost: 0.026668878\nEpoch: 695, Cost: 0.026265791\nEpoch: 696, Cost: 0.025773080\nEpoch: 697, Cost: 0.025914166\nEpoch: 698, Cost: 0.026304122\nEpoch: 699, Cost: 0.026201224\nEpoch: 700, Cost: 0.025793882\nEpoch: 701, Cost: 0.025646944\nEpoch: 702, Cost: 0.025709679\nEpoch: 703, Cost: 0.025641393\nEpoch: 704, Cost: 0.025524866\nEpoch: 705, Cost: 0.025640102\nEpoch: 706, Cost: 0.025897630\nEpoch: 707, Cost: 0.026045181\nEpoch: 708, Cost: 0.026141440\nEpoch: 709, Cost: 0.026377508\nEpoch: 710, Cost: 0.026509073\nEpoch: 711, Cost: 0.026422692\nEpoch: 712, Cost: 0.026007893\nEpoch: 713, Cost: 0.025692377\nEpoch: 714, Cost: 0.025526103\nEpoch: 715, Cost: 0.025527239\nEpoch: 716, Cost: 0.025700510\nEpoch: 717, Cost: 0.026068944\nEpoch: 718, Cost: 0.026419828\nEpoch: 719, Cost: 0.026533026\nEpoch: 720, Cost: 0.026285753\nEpoch: 721, Cost: 0.025999678\nEpoch: 722, Cost: 0.025947154\nEpoch: 723, Cost: 0.026315209\nEpoch: 724, Cost: 0.026906915\nEpoch: 725, Cost: 0.027299615\nEpoch: 726, Cost: 0.027102737\nEpoch: 727, Cost: 0.026325906\nEpoch: 728, Cost: 0.025707107\nEpoch: 729, Cost: 0.025849715\nEpoch: 730, Cost: 0.026663145\nEpoch: 731, Cost: 0.027335616\nEpoch: 732, Cost: 0.027295211\nEpoch: 733, Cost: 0.026812583\nEpoch: 734, Cost: 0.026432790\nEpoch: 735, Cost: 0.026275134\nEpoch: 736, Cost: 0.025974706\nEpoch: 737, Cost: 0.025344113\nEpoch: 738, Cost: 0.024925191\nEpoch: 739, Cost: 0.024903260\nEpoch: 740, Cost: 0.024907479\nEpoch: 741, Cost: 0.024516370\nEpoch: 742, Cost: 0.024014071\nEpoch: 743, Cost: 0.023941794\nEpoch: 744, Cost: 0.024189167\nEpoch: 745, Cost: 0.024195362\nEpoch: 746, Cost: 0.023824511\nEpoch: 747, Cost: 0.023485847\nEpoch: 748, Cost: 0.023497112\nEpoch: 749, Cost: 0.023599463\nEpoch: 750, Cost: 0.023470230\nEpoch: 751, Cost: 0.023174303\nEpoch: 752, Cost: 0.023024019\nEpoch: 753, Cost: 0.023072271\nEpoch: 754, Cost: 0.023114894\nEpoch: 755, Cost: 0.023021847\nEpoch: 756, Cost: 0.022901213\nEpoch: 757, Cost: 0.022870718\nEpoch: 758, Cost: 0.022893056\nEpoch: 759, Cost: 0.022876265\nEpoch: 760, Cost: 0.022845818\nEpoch: 761, Cost: 0.022906639\nEpoch: 762, Cost: 0.023142986\nEpoch: 763, Cost: 0.023537958\nEpoch: 764, Cost: 0.024187800\nEpoch: 765, Cost: 0.025204053\nEpoch: 766, Cost: 0.026857743\nEpoch: 767, Cost: 0.028787458\nEpoch: 768, Cost: 0.030244339\nEpoch: 769, Cost: 0.029570976\nEpoch: 770, Cost: 0.027274385\nEpoch: 771, Cost: 0.026302569\nEpoch: 772, Cost: 0.027811361\nEpoch: 773, Cost: 0.028851932\nEpoch: 774, Cost: 0.026121726\nEpoch: 775, Cost: 0.023526786\nEpoch: 776, Cost: 0.024516108\nEpoch: 777, Cost: 0.025591064\nEpoch: 778, Cost: 0.024172433\nEpoch: 779, Cost: 0.023304421\nEpoch: 780, Cost: 0.024217896\nEpoch: 781, Cost: 0.023781138\nEpoch: 782, Cost: 0.022606727\nEpoch: 783, Cost: 0.023061249\nEpoch: 784, Cost: 0.023314137\nEpoch: 785, Cost: 0.022366587\nEpoch: 786, Cost: 0.022302378\nEpoch: 787, Cost: 0.022813676\nEpoch: 788, Cost: 0.022351880\nEpoch: 789, Cost: 0.021996399\nEpoch: 790, Cost: 0.022339880\nEpoch: 791, Cost: 0.022208640\nEpoch: 792, Cost: 0.021851724\nEpoch: 793, Cost: 0.022036951\nEpoch: 794, Cost: 0.022088496\nEpoch: 795, Cost: 0.021784237\nEpoch: 796, Cost: 0.021787779\nEpoch: 797, Cost: 0.021967735\nEpoch: 798, Cost: 0.021910409\nEpoch: 799, Cost: 0.021959372\nEpoch: 800, Cost: 0.022246804\nEpoch: 801, Cost: 0.022450253\nEpoch: 802, Cost: 0.022642490\nEpoch: 803, Cost: 0.023145132\nEpoch: 804, Cost: 0.023722328\nEpoch: 805, Cost: 0.024117550\nEpoch: 806, Cost: 0.024246832\nEpoch: 807, Cost: 0.023941074\nEpoch: 808, Cost: 0.023022458\nEpoch: 809, Cost: 0.021986058\nEpoch: 810, Cost: 0.021473179\nEpoch: 811, Cost: 0.021597553\nEpoch: 812, Cost: 0.022139540\nEpoch: 813, Cost: 0.022770578\nEpoch: 814, Cost: 0.023158303\nEpoch: 815, Cost: 0.023164427\nEpoch: 816, Cost: 0.023116339\nEpoch: 817, Cost: 0.023292309\nEpoch: 818, Cost: 0.023714077\nEpoch: 819, Cost: 0.024003899\nEpoch: 820, Cost: 0.023845665\nEpoch: 821, Cost: 0.023240484\nEpoch: 822, Cost: 0.022556800\nEpoch: 823, Cost: 0.022329394\nEpoch: 824, Cost: 0.022586759\nEpoch: 825, Cost: 0.022827011\nEpoch: 826, Cost: 0.022617958\nEpoch: 827, Cost: 0.022057803\nEpoch: 828, Cost: 0.021618513\nEpoch: 829, Cost: 0.021535052\nEpoch: 830, Cost: 0.021601243\nEpoch: 831, Cost: 0.021558372\nEpoch: 832, Cost: 0.021354303\nEpoch: 833, Cost: 0.021203004\nEpoch: 834, Cost: 0.021225991\nEpoch: 835, Cost: 0.021287428\nEpoch: 836, Cost: 0.021284692\nEpoch: 837, Cost: 0.021301664\nEpoch: 838, Cost: 0.021541832\nEpoch: 839, Cost: 0.022080222\nEpoch: 840, Cost: 0.022711683\nEpoch: 841, Cost: 0.023373185\nEpoch: 842, Cost: 0.023880977\nEpoch: 843, Cost: 0.024260404\nEpoch: 844, Cost: 0.024076715\nEpoch: 845, Cost: 0.023268444\nEpoch: 846, Cost: 0.022175094\nEpoch: 847, Cost: 0.021693898\nEpoch: 848, Cost: 0.022204144\nEpoch: 849, Cost: 0.023046151\nEpoch: 850, Cost: 0.023088377\nEpoch: 851, Cost: 0.022093186\nEpoch: 852, Cost: 0.021019286\nEpoch: 853, Cost: 0.020859642\nEpoch: 854, Cost: 0.021320520\nEpoch: 855, Cost: 0.021530602\nEpoch: 856, Cost: 0.021173455\nEpoch: 857, Cost: 0.020869065\nEpoch: 858, Cost: 0.021032002\nEpoch: 859, Cost: 0.021302456\nEpoch: 860, Cost: 0.021281146\nEpoch: 861, Cost: 0.021085007\nEpoch: 862, Cost: 0.021214236\nEpoch: 863, Cost: 0.021672040\nEpoch: 864, Cost: 0.022048259\nEpoch: 865, Cost: 0.022104578\nEpoch: 866, Cost: 0.021931961\nEpoch: 867, Cost: 0.021777462\nEpoch: 868, Cost: 0.021594495\nEpoch: 869, Cost: 0.021332396\nEpoch: 870, Cost: 0.021246558\nEpoch: 871, Cost: 0.021450223\nEpoch: 872, Cost: 0.021761592\nEpoch: 873, Cost: 0.021812657\nEpoch: 874, Cost: 0.021458499\nEpoch: 875, Cost: 0.020988215\nEpoch: 876, Cost: 0.020604894\nEpoch: 877, Cost: 0.020369936\nEpoch: 878, Cost: 0.020228116\nEpoch: 879, Cost: 0.020170627\nEpoch: 880, Cost: 0.020136522\nEpoch: 881, Cost: 0.020032402\nEpoch: 882, Cost: 0.019847048\nEpoch: 883, Cost: 0.019685160\nEpoch: 884, Cost: 0.019667899\nEpoch: 885, Cost: 0.019737251\nEpoch: 886, Cost: 0.019819235\nEpoch: 887, Cost: 0.019886639\nEpoch: 888, Cost: 0.020086640\nEpoch: 889, Cost: 0.020427397\nEpoch: 890, Cost: 0.020986801\nEpoch: 891, Cost: 0.021513464\nEpoch: 892, Cost: 0.022107614\nEpoch: 893, Cost: 0.022311406\nEpoch: 894, Cost: 0.022225097\nEpoch: 895, Cost: 0.021455985\nEpoch: 896, Cost: 0.020577308\nEpoch: 897, Cost: 0.020017404\nEpoch: 898, Cost: 0.020140428\nEpoch: 899, Cost: 0.020693481\nEpoch: 900, Cost: 0.021055555\nEpoch: 901, Cost: 0.021157872\nEpoch: 902, Cost: 0.021127749\nEpoch: 903, Cost: 0.021463463\nEpoch: 904, Cost: 0.021947127\nEpoch: 905, Cost: 0.022074711\nEpoch: 906, Cost: 0.021478005\nEpoch: 907, Cost: 0.020521656\nEpoch: 908, Cost: 0.019912632\nEpoch: 909, Cost: 0.019902054\nEpoch: 910, Cost: 0.020122079\nEpoch: 911, Cost: 0.020246873\nEpoch: 912, Cost: 0.020192500\nEpoch: 913, Cost: 0.020125808\nEpoch: 914, Cost: 0.020030415\nEpoch: 915, Cost: 0.019904587\nEpoch: 916, Cost: 0.019838275\nEpoch: 917, Cost: 0.019926814\nEpoch: 918, Cost: 0.019973489\nEpoch: 919, Cost: 0.019854218\nEpoch: 920, Cost: 0.019506639\nEpoch: 921, Cost: 0.019294083\nEpoch: 922, Cost: 0.019297997\nEpoch: 923, Cost: 0.019322792\nEpoch: 924, Cost: 0.019158656\nEpoch: 925, Cost: 0.018861946\nEpoch: 926, Cost: 0.018706972\nEpoch: 927, Cost: 0.018751033\nEpoch: 928, Cost: 0.018862130\nEpoch: 929, Cost: 0.018896731\nEpoch: 930, Cost: 0.018948004\nEpoch: 931, Cost: 0.019156788\nEpoch: 932, Cost: 0.019589953\nEpoch: 933, Cost: 0.020132413\nEpoch: 934, Cost: 0.020721141\nEpoch: 935, Cost: 0.021161921\nEpoch: 936, Cost: 0.021390961\nEpoch: 937, Cost: 0.021010289\nEpoch: 938, Cost: 0.020164143\nEpoch: 939, Cost: 0.019210149\nEpoch: 940, Cost: 0.018823072\nEpoch: 941, Cost: 0.019092135\nEpoch: 942, Cost: 0.019462269\nEpoch: 943, Cost: 0.019374616\nEpoch: 944, Cost: 0.018826002\nEpoch: 945, Cost: 0.018454790\nEpoch: 946, Cost: 0.018653654\nEpoch: 947, Cost: 0.019156745\nEpoch: 948, Cost: 0.019411070\nEpoch: 949, Cost: 0.019262604\nEpoch: 950, Cost: 0.019051451\nEpoch: 951, Cost: 0.019113671\nEpoch: 952, Cost: 0.019337494\nEpoch: 953, Cost: 0.019388624\nEpoch: 954, Cost: 0.019151198\nEpoch: 955, Cost: 0.018872149\nEpoch: 956, Cost: 0.018724121\nEpoch: 957, Cost: 0.018685676\nEpoch: 958, Cost: 0.018632321\nEpoch: 959, Cost: 0.018611314\nEpoch: 960, Cost: 0.018748064\nEpoch: 961, Cost: 0.019065749\nEpoch: 962, Cost: 0.019352430\nEpoch: 963, Cost: 0.019483807\nEpoch: 964, Cost: 0.019354610\nEpoch: 965, Cost: 0.019132830\nEpoch: 966, Cost: 0.018855920\nEpoch: 967, Cost: 0.018616645\nEpoch: 968, Cost: 0.018445082\nEpoch: 969, Cost: 0.018400626\nEpoch: 970, Cost: 0.018444825\nEpoch: 971, Cost: 0.018495791\nEpoch: 972, Cost: 0.018454978\nEpoch: 973, Cost: 0.018376116\nEpoch: 974, Cost: 0.018389890\nEpoch: 975, Cost: 0.018637273\nEpoch: 976, Cost: 0.019116521\nEpoch: 977, Cost: 0.019787423\nEpoch: 978, Cost: 0.020495752\nEpoch: 979, Cost: 0.021161525\nEpoch: 980, Cost: 0.021584529\nEpoch: 981, Cost: 0.021480385\nEpoch: 982, Cost: 0.020887418\nEpoch: 983, Cost: 0.019957365\nEpoch: 984, Cost: 0.019296130\nEpoch: 985, Cost: 0.019026581\nEpoch: 986, Cost: 0.018968919\nEpoch: 987, Cost: 0.018830486\nEpoch: 988, Cost: 0.018605536\nEpoch: 989, Cost: 0.018454706\nEpoch: 990, Cost: 0.018296264\nEpoch: 991, Cost: 0.018041022\nEpoch: 992, Cost: 0.017824411\nEpoch: 993, Cost: 0.017842790\nEpoch: 994, Cost: 0.017923163\nEpoch: 995, Cost: 0.017753899\nEpoch: 996, Cost: 0.017393695\nEpoch: 997, Cost: 0.017250558\nEpoch: 998, Cost: 0.017441975\nEpoch: 999, Cost: 0.017615896\nEpoch: 1000, Cost: 0.017462082\n\u001b[1m 1/32\u001b[0m \u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 273ms/step - loss: 0.0183","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1719737821.847025     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0177 \nTotal loss:  0.017482249066233635\n\u001b[1m 1/32\u001b[0m \u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 180ms/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1719737822.280030     117 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport pandas as pd\nimport tensorflow as tf\nimport kerastuner.tuners as kt\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.losses import MeanSquaredLogarithmicError\n\n# data in google colab\nTRAIN_DATA_PATH = '/content/sample_data/california_housing_train.csv'\nTEST_DATA_PATH = '/content/sample_data/california_housing_test.csv'\nTARGET_NAME = 'median_house_value'\n\ntrain_data = pd.read_csv(TRAIN_DATA_PATH)\ntest_data = pd.read_csv(TEST_DATA_PATH)\n\nx_train, y_train = train_data.drop(TARGET_NAME, axis=1), train_data[TARGET_NAME]\nx_test, y_test = test_data.drop(TARGET_NAME, axis=1), test_data[TARGET_NAME]\n\n#Scale the dataset using MinMaxScaler.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef scale_datasets(x_train, x_test):\n  \"\"\"\n  Standard Scale test and train data\n  \"\"\"\n  standard_scaler = MinMaxScaler()\n  x_train_scaled = pd.DataFrame(\n      standard_scaler.fit_transform(x_train),\n      columns=x_train.columns\n  )\n  x_test_scaled = pd.DataFrame(\n      standard_scaler.transform(x_test),\n      columns = x_test.columns\n  )\n  return x_train_scaled, x_test_scaled\n  \nx_train_scaled, x_test_scaled = scale_datasets(x_train, x_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:51:23.894500Z","iopub.execute_input":"2024-06-30T08:51:23.895566Z","iopub.status.idle":"2024-06-30T08:51:25.402822Z","shell.execute_reply.started":"2024-06-30T08:51:23.895518Z","shell.execute_reply":"2024-06-30T08:51:25.401241Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3626060163.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n  import kerastuner.tuners as kt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m TEST_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/sample_data/california_housing_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m TARGET_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_house_value\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 18\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(TEST_DATA_PATH)\n\u001b[1;32m     21\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mdrop(TARGET_NAME, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), train_data[TARGET_NAME]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/california_housing_train.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/sample_data/california_housing_train.csv'","output_type":"error"}]},{"cell_type":"code","source":"class AutoEncoders(Model):\n\n  def __init__(self, output_units):\n\n    super().__init__()\n    self.encoder = Sequential(\n        [\n          Dense(32, activation=\"relu\"),\n          Dense(16, activation=\"relu\"),\n          Dense(7, activation=\"relu\")\n        ]\n    )\n\n    self.decoder = Sequential(\n        [\n          Dense(16, activation=\"relu\"),\n          Dense(32, activation=\"relu\"),\n          Dense(output_units, activation=\"sigmoid\")\n        ]\n    )\n\ndef call(self, inputs):\n\n  encoded = self.encoder(inputs)\n  decoded = self.decoder(encoded)\n  return decoded\n  \nauto_encoder = AutoEncoders(len(x_train_scaled.columns))\n\nauto_encoder.compile(\n    loss='mae',\n    metrics=['mae'],\n    optimizer='adam'\n)\n\nhistory = auto_encoder.fit(\n    x_train_scaled, \n    x_train_scaled, \n    epochs=15, \n    batch_size=32, \n    validation_data=(x_test_scaled, x_test_scaled)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_layer = auto_encoder.get_layer('sequential')\nreduced_df = pd.DataFrame(encoder_layer.predict(x_train_scaled))\nreduced_df = reduced_df.add_prefix('feature_')","metadata":{},"execution_count":null,"outputs":[]}]}