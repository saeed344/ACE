{"metadata":{"colab":{"provenance":[],"mount_file_id":"1KQ8NiBVlnI9DFREXghMOW5uyRB-T4L78","authorship_tag":"ABX9TyOVMoxQZ4WFMk9q734RB8DU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8922343,"sourceType":"datasetVersion","datasetId":5298654}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1zHya1UmYv7","executionInfo":{"status":"ok","timestamp":1685293790270,"user_tz":-300,"elapsed":64486,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"361be0ef-5b58-43fc-a0db-cb42ec760a7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n__dn2v3pLT8","executionInfo":{"status":"ok","timestamp":1685294164802,"user_tz":-300,"elapsed":10430,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"7a9e7806-c237-4721-e659-71cae60ed993","execution":{"iopub.status.busy":"2024-07-10T12:57:00.186955Z","iopub.execute_input":"2024-07-10T12:57:00.187701Z","iopub.status.idle":"2024-07-10T12:57:12.232292Z","shell.execute_reply.started":"2024-07-10T12:57:00.187671Z","shell.execute_reply":"2024-07-10T12:57:12.231195Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom gensim.models import FastText\nimport numpy as np\nimport torch\nfrom transformers import BertModel, BertTokenizer\n\nimport pandas as pd\nimport numpy as np\nimport re, os, sys\nfrom itertools import product\n\ndef read_nucleotide_sequences(file):\n    if os.path.exists(file) == False:\n        print('Error: file %s does not exist.' % file)\n        sys.exit(1)\n    with open(file) as f:\n        records = f.read()\n    if re.search('>', records) == None:\n        print('Error: the input file %s seems not in FASTA format!' % file)\n        sys.exit(1)\n    records = records.split('>')[1:]\n    fasta_sequences = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        header, sequence = array[0].split()[0], re.sub('[^ACGTU-]', '-', ''.join(array[1:]).upper())\n        header_array = header.split('|')\n        name = header_array[0]\n        label = header_array[1] if len(header_array) >= 2 else '0'\n        label_train = header_array[2] if len(header_array) >= 3 else 'training'\n        sequence = re.sub('U', 'T', sequence)\n        fasta_sequences.append(sequence)\n    return fasta_sequences\n\n#!/usr/bin/env python\n#_*_coding:utf-8_*_\n\nimport re\n\ndef check_fasta_with_equal_length(fastas):\n    status = True\n    lenList = set()\n    for i in fastas:\n        lenList.add(len(i[1]))\n    if len(lenList) == 1:\n        return True\n    else:\n        return False\n\ndef get_min_sequence_length(fastas):\n    minLen = 10000\n    for i in fastas:\n        if minLen > len(i[1]):\n            minLen = len(i[1])\n    return minLen\n\ndef get_min_sequence_length_1(fastas):\n    minLen = 10000\n    for i in fastas:\n        if minLen > len(re.sub('-', '', i[1])):\n            minLen = len(re.sub('-', '', i[1]))\n    return minLen\ndef readFasta(file):\n    if os.path.exists(file) == False:\n        print('Error: \"' + file + '\" does not exist.')\n        sys.exit(1)\n\n    with open(file) as f:\n        records = f.read()\n\n    if re.search('>', records) == None:\n        print('The input file seems not in fasta format.')\n        sys.exit(1)\n\n    records = records.split('>')[1:]\n    myFasta = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n        myFasta.append([name, sequence])\n    return myFasta\n\n\ndef extract_features(dna_sequences, vector_size=100, window=5, min_count=1):\n    # Prepare data for FastText\n    tokenized_sequences = [list(sequence) for sequence in dna_sequences]\n\n    # Train FastText model\n    model = FastText(tokenized_sequences, vector_size=vector_size, window=window, min_count=min_count)\n\n    # Extract features\n    features = np.zeros((len(dna_sequences), vector_size))\n    for i, sequence in enumerate(tokenized_sequences):\n        for token in sequence:\n            features[i] += model.wv[token]\n        features[i] /= len(sequence)\n\n    return features\n\n# Example usage\ndna_sequences = ['ATCGATCGATCG', 'CGATCGATCGATCG']\n\nfastas = read_nucleotide_sequences('/content/drive/MyDrive/Research Work/NLP-Features code/balanced_training_datasets/cd_ac4c_training_10.fasta')\nfeatures = extract_features(fastas)\ndata_csv=pd.DataFrame(features)\ndata_csv.to_csv('/content/drive/MyDrive/Research Work/NLP-Features code/balanced_training_datasets/FastText_feature_training_10.csv')\n\nprint(features)","metadata":{"id":"HnpLO1z5mIed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fasttext","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCQVzFi0-FQW","executionInfo":{"status":"ok","timestamp":1714765382584,"user_tz":-120,"elapsed":69271,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"756a4c69-0953-44b0-8289-10b72e37fe63","execution":{"iopub.status.busy":"2024-07-10T13:01:01.227541Z","iopub.execute_input":"2024-07-10T13:01:01.227990Z","iopub.status.idle":"2024-07-10T13:01:13.218976Z","shell.execute_reply.started":"2024-07-10T13:01:01.227956Z","shell.execute_reply":"2024-07-10T13:01:13.217594Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.2)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.12.0)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fasttext) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install biopython","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oFKHmPyuAHt_","executionInfo":{"status":"ok","timestamp":1714765744187,"user_tz":-120,"elapsed":9803,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"309699d7-dafc-48ad-d908-3a78b457ba6e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"Collecting biopython\n\n  Downloading biopython-1.83-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n\nInstalling collected packages: biopython\n\nSuccessfully installed biopython-1.83\n"}]},{"cell_type":"code","source":"import fasttext\n\n# Path to your RNA sequence text file\ndata_path = \"/content/drive/MyDrive/Research Work/NLP-Features code/Saweera_data/balanced_training_datasets/cd_ac4c_training_1.fasta\"\n\n# Train FastText model\nmodel = fasttext.train_unsupervised(data_path, model='cbow', dim=100, epoch=10, minCount=1)\n\n# Save the trained model\nmodel.save_model(\"rna_fasttext_model.bin\")\n\nprint(\"Model trained and saved successfully.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0buyVlmu_Rwq","executionInfo":{"status":"ok","timestamp":1714765674117,"user_tz":-120,"elapsed":5475,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"8ea4616a-5de0-4d1e-b7ff-98664610d934"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Model trained and saved successfully.\n"}]},{"cell_type":"code","source":"import fasttext\nfrom Bio import SeqIO\nimport numpy as np\nimport pandas as pd\n\n# Load FastText model\n# model = fasttext.load_model('/content/rna_fasttext_model.bin')  # Pre-trained FastText model, you can replace it with your own trained model\n# Train FastText model\nmodel = fasttext.train_unsupervised(input_file, model='cbow', dim=100, epoch=10, minCount=1)\n\n# Function to convert RNA sequence into numerical features\ndef sequence_to_features(sequence, model):\n    # Tokenize RNA sequence into k-mers (n-grams)\n    k = 3  # You can adjust the k-mer size\n    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n\n    # Compute FastText embeddings for each k-mer\n    embeddings = [model.get_word_vector(kmer) for kmer in kmers]\n\n    # Calculate mean embedding as the feature representation\n    if embeddings:\n        feature = np.mean(embeddings, axis=0)\n    else:\n        feature = np.zeros(model.get_dimension())  # Return zero vector if no k-mers found\n\n    return feature\n\n# Function to read RNA sequences from FASTA file and convert to features\ndef fasta_to_features(input_file, model):\n    sequences = []\n    features = []\n\n    for record in SeqIO.parse(input_file, \"fasta\"):\n        sequences.append(str(record.seq))\n        features.append(sequence_to_features(str(record.seq), model))\n\n    return sequences, np.array(features)\n\n# Read RNA sequences from FASTA file and convert to features\ninput_file = \"/content/drive/MyDrive/Research Work/NLP-Features code/Saweera_data/balanced_training_datasets/cd_ac4c_training_1.fasta\"\nsequences, features = fasta_to_features(input_file, model)\n\n# Create DataFrame for features\ncolumns = [f\"feature_{i+1}\" for i in range(features.shape[1])]\ndf = pd.DataFrame(features, columns=columns)\ndf.insert(0, \"Sequence\", sequences)\n\n# Save features as CSV\noutput_file = \"rna_features.csv\"\ndf.to_csv(output_file, index=False)\n\nprint(\"Features saved to\", output_file)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqQ-h15b-QnD","executionInfo":{"status":"ok","timestamp":1714766181951,"user_tz":-120,"elapsed":11917,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"f079094d-36e1-42bb-98e2-8e25dc474e3b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Features saved to rna_features.csv\n"}]},{"cell_type":"markdown","source":"","metadata":{"id":"hAcf2GgqmXSk"}},{"cell_type":"code","source":"!pip install gensim \n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:01:28.088865Z","iopub.execute_input":"2024-07-10T13:01:28.089302Z","iopub.status.idle":"2024-07-10T13:01:39.923663Z","shell.execute_reply.started":"2024-07-10T13:01:28.089270Z","shell.execute_reply":"2024-07-10T13:01:39.922389Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.11.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.4.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import FastText\n\ndef read_peptide_sequences(file):\n    if not os.path.exists(file):\n        print(f'Error: file {file} does not exist.')\n        sys.exit(1)\n    \n    with open(file) as f:\n        records = f.read()\n    \n    if '>' not in records:\n        print(f'Error: the input file {file} seems not in FASTA format!')\n        sys.exit(1)\n    \n    records = records.split('>')[1:]\n    peptide_sequences = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        header, sequence = array[0], ''.join(array[1:]).upper()\n        peptide_sequences.append(sequence)\n    \n    return peptide_sequences\n\ndef extract_features(peptide_sequences, vector_size=100, window=5, min_count=1):\n    # Prepare data for FastText\n    tokenized_sequences = [list(sequence) for sequence in peptide_sequences]\n\n    # Train FastText model\n    model = FastText(tokenized_sequences, vector_size=vector_size, window=window, min_count=min_count)\n\n    # Extract features\n    features = np.zeros((len(peptide_sequences), vector_size))\n    for i, sequence in enumerate(tokenized_sequences):\n        for token in sequence:\n            features[i] += model.wv[token]\n        features[i] /= len(sequence)\n\n    return features\n\ndef main():\n    # File paths\n    input_file = '/kaggle/input/ace-dataset/ACE_full_dataset.txt'\n    output_file = '/kaggle/working/fasttext_features_ACE.csv'\n    \n    # Read peptide sequences\n    peptide_sequences = read_peptide_sequences(input_file)\n    \n    # Extract features using FastText\n    features = extract_features(peptide_sequences)\n    \n    # Save features to CSV\n    features_df = pd.DataFrame(features)\n    features_df.to_csv(output_file, index=False)\n    \n    print(\"Features extracted and saved to CSV successfully.\")\n    print(features)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:02:22.166426Z","iopub.execute_input":"2024-07-10T13:02:22.167435Z","iopub.status.idle":"2024-07-10T13:02:36.964883Z","shell.execute_reply.started":"2024-07-10T13:02:22.167398Z","shell.execute_reply":"2024-07-10T13:02:36.963790Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Features extracted and saved to CSV successfully.\n[[-0.0782537   0.04756918  0.04022702 ...  0.09361638  0.01984505\n   0.01214317]\n [-0.07814427  0.04698247  0.04004453 ...  0.09273018  0.01868749\n   0.01251137]\n [-0.07606108  0.04613785  0.0391121  ...  0.08946893  0.01659791\n   0.01310674]\n ...\n [-0.09876951  0.06239232  0.04844261 ...  0.11263584  0.0259387\n   0.01808453]\n [-0.09526661  0.05825514  0.04923445 ...  0.11082024  0.02411642\n   0.01881375]\n [-0.09460388  0.06089057  0.04749231 ...  0.11063424  0.0245214\n   0.01726293]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Word 2 vectore feature using CWOB","metadata":{}},{"cell_type":"code","source":"\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom collections import Counter\n\n\ndef read_peptide_sequences(file):\n    if not os.path.exists(file):\n        print(f'Error: file {file} does not exist.')\n        sys.exit(1)\n    \n    with open(file) as f:\n        records = f.read()\n    \n    if '>' not in records:\n        print(f'Error: the input file {file} seems not in FASTA format!')\n        sys.exit(1)\n    \n    records = records.split('>')[1:]\n    peptide_sequences = []\n    for fasta in records:\n        array = fasta.split('\\n')\n        header, sequence = array[0], ''.join(array[1:]).upper()\n        peptide_sequences.append(sequence)\n    \n    return peptide_sequences\n\ndef extract_features(peptide_sequences, vector_size=100, window=5, min_count=1):\n    # Prepare data for Word2Vec\n    tokenized_sequences = [list(sequence) for sequence in peptide_sequences]\n\n    # Train Word2Vec model\n    model = Word2Vec(tokenized_sequences, vector_size=vector_size, window=window, min_count=min_count)\n    \n    # Create a vocabulary list\n    vocabulary = list(model.wv.index_to_key)\n\n    # Extract BoW + Word2Vec features\n    features = []\n    for sequence in tokenized_sequences:\n        # Bag of Words representation\n        bow = Counter(sequence)\n        bow_vector = [bow[token] for token in vocabulary]\n        \n        # Word2Vec representation\n        word2vec_vector = np.zeros(vector_size)\n        for token in sequence:\n            if token in model.wv:\n                word2vec_vector += model.wv[token]\n        word2vec_vector /= len(sequence)\n        \n        # Combine BoW and Word2Vec vectors\n        combined_vector = np.concatenate([bow_vector, word2vec_vector])\n        features.append(combined_vector)\n    \n    return np.array(features), vocabulary\n\ndef main():\n    # File paths\n    input_file = '/kaggle/input/ace-dataset/ACE_full_dataset.txt'\n    output_file = '/kaggle/working/bow_word2vec_features_ACE.csv'\n    \n    # Read peptide sequences\n    peptide_sequences = read_peptide_sequences(input_file)\n    \n    # Extract features using BoW + Word2Vec\n    features, vocabulary = extract_features(peptide_sequences)\n    \n    # Create a DataFrame with the combined features\n    bow_columns = [f'bow_{token}' for token in vocabulary]\n    word2vec_columns = [f'word2vec_{i}' for i in range(features.shape[1] - len(vocabulary))]\n    columns = bow_columns + word2vec_columns\n    \n    features_df = pd.DataFrame(features, columns=columns)\n    features_df.to_csv(output_file, index=False)\n    \n    print(\"Features extracted and saved to CSV successfully.\")\n    print(features)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:07:24.832866Z","iopub.execute_input":"2024-07-10T13:07:24.833645Z","iopub.status.idle":"2024-07-10T13:07:25.091893Z","shell.execute_reply.started":"2024-07-10T13:07:24.833606Z","shell.execute_reply":"2024-07-10T13:07:25.090893Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Features extracted and saved to CSV successfully.\n[[ 0.          1.          0.         ... -0.01750997  0.06654619\n   0.0491765 ]\n [ 0.          1.          0.         ... -0.01652273  0.06486657\n   0.04985909]\n [ 0.          1.          0.         ... -0.01658617  0.06147339\n   0.05035048]\n ...\n [ 1.          0.          0.         ... -0.02397426  0.08200689\n   0.06096922]\n [ 1.          1.          0.         ... -0.02460871  0.0787589\n   0.06186177]\n [ 1.          1.          0.         ... -0.02208124  0.0797771\n   0.05769714]]\n","output_type":"stream"}]}]}