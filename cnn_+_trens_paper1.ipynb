{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":8850732,"datasetId":5298654,"databundleVersionId":9010325}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-04T20:51:27.094572Z","iopub.execute_input":"2024-07-04T20:51:27.095103Z","iopub.status.idle":"2024-07-04T20:51:27.113085Z","shell.execute_reply.started":"2024-07-04T20:51:27.095076Z","shell.execute_reply":"2024-07-04T20:51:27.112126Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/ace-dataset/ACE_dataset.csv\n/kaggle/input/ace-dataset/ACE_dataset.fasta\n/kaggle/input/ace-dataset/features/ACE_ASDC.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_1_features.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_2_features.csv\n/kaggle/input/ace-dataset/features/esmv1_feat_ACE.csv\n/kaggle/input/ace-dataset/features/ACE_embeddings_prot_t5_xl_bfd.csv\n/kaggle/input/ace-dataset/features/esm2_t6_8M_feat_ACE.csv\n/kaggle/input/ace-dataset/features/opf_7bit_type_3_features.csv\n/kaggle/input/ace-dataset/features/opf_10bit_features.csv\n/kaggle/input/ace-dataset/features/ACE_AAC.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-500.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-300.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-50.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-400.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-450.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-350.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-250.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-200.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-100.csv\n/kaggle/input/ace-dataset/auto_enco_feat/AEDN-150.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score, matthews_corrcoef\nfrom keras.saving import register_keras_serializable\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.preprocessing import scale\n\n# Load data\npath = \"/kaggle/input/ace-dataset/auto_enco_feat/\"\ndata_ = pd.read_csv(path + 'AEDN-100.csv')\n\ndata_np = np.array(data_)\ndata = scale(data_np[:, 1:])\n\nlabel1 = np.ones((394, 1))  # Value can be changed\nlabel2 = np.zeros((626, 1))\nlabels = np.append(label1, label2)\n\n# Define the positional encoding function\ndef positional_encoding(positions, d):\n    pos = np.arange(positions)[:, np.newaxis]\n    k = np.arange(d)[np.newaxis, :]\n    i = k // 2\n    angle_rads = pos / (10000 ** (2 * i / d))\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n@register_keras_serializable()\nclass TransformerModel(keras.Model):\n    def __init__(self, input_vocab_size, d_model, num_heads, ff_dim, rate=0.1, maxlen=50):\n        super(TransformerModel, self).__init__()\n\n        self.embedding = layers.Embedding(input_vocab_size, d_model)\n        self.PE = positional_encoding(maxlen, d_model)\n        self.transformer_block = TransformerBlock_Encode(d_model, num_heads, ff_dim, rate)\n        self.transformer_block2 = TransformerBlock_decode(d_model, num_heads, ff_dim, rate)\n        self.flatten = layers.Flatten()\n        self.fc1 = layers.Dense(512, activation=\"relu\")\n        self.fc3 = layers.Dense(256, activation=\"relu\")\n        self.fc2 = layers.Dense(1, activation=\"sigmoid\")\n\n    def call(self, inputs, training):\n        x = self.embedding(inputs)\n        y = self.transformer_block(x)\n        x = self.transformer_block2(x, y, y)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc3(x)\n        return self.fc2(x)\n    \n    @classmethod\n    def from_config(cls, config):\n        return cls(**config['config'])\n\nclass TransformerBlock_decode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_decode, self).__init__()\n\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n        self.dropout3 = layers.Dropout(rate)\n\n    def call(self, inputs, q, k, training=None):\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n        attn_output1 = self.att1(q, k, out1)\n        out2 = self.layernorm2(out1 + self.dropout2(attn_output1, training=training))\n\n        ffn_output = self.ffn(out2)\n        return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n\n\nclass TransformerBlock_Encode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_Encode, self).__init__()\n        self.con = layers.Conv1D(256, 5, padding='same')\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training=None):\n        inputs = self.con(inputs)\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n        ffn_output = self.ffn(out1)\n        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n\n# Define the model parameters\ninput_vocab_size = 100  # Replace with the actual vocabulary size\nd_model = 256\nnum_heads = 4\nff_dim = 128\ninitial_learning_rate = 0.01\n\n# Initialize KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n# Lists to store results\nfold_acc = []\nfold_loss = []\nfold_y_score = []\nfold_y_test = []\n\n# 5-fold cross validation\nfor fold, (train_index, val_index) in enumerate(kf.split(data)):\n    print(f\"Training fold {fold + 1}...\")\n\n    X_train, X_val = data[train_index], data[val_index]\n    y_train, y_val = labels[train_index], labels[val_index]\n\n    model = TransformerModel(input_vocab_size, d_model, num_heads, ff_dim)\n    optimizer = Adam(learning_rate=initial_learning_rate, clipvalue=0.5)\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n\n    save_dir = '/kaggle/working/'\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n    model_checkpoint = ModelCheckpoint(os.path.join(save_dir, f'model_fold_{fold + 1}.keras'),\n                                       save_best_only=True,\n                                       monitor='val_loss',\n                                       mode='min')\n    callbacks = [model_checkpoint, early_stopping, reduce_lr]\n\n    history = model.fit(X_train, y_train, epochs=100, batch_size=8, validation_data=(X_val, y_val),\n                        callbacks=callbacks, verbose=1, shuffle=True)\n    \n    # Load the model with custom objects\n#     best_model = keras.models.load_model(os.path.join(save_dir, f'model_fold_{fold + 1}.keras'), custom_objects={'TransformerModel': TransformerModel})\n\n    y_score = model.predict(X_val).ravel()  # Predicted scores\n    y_pred = (y_score > 0.5).astype(int)  # Predicted labels\n    \n    # Store y_score and y_test for metrics calculation\n    fold_y_score.append(y_score)\n    fold_y_test.append(y_val)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_val, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n    sensitivity = tp / (tp + fn)\n    specificity = tn / (tn + fp)\n    mcc = matthews_corrcoef(y_val, y_pred)\n    f1 = f1_score(y_val, y_pred)\n    auc = roc_auc_score(y_val, y_score)\n    \n    print(f\"Fold {fold + 1} Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Sensitivity: {sensitivity:.4f}\")\n    print(f\"Specificity: {specificity:.4f}\")\n    print(f\"MCC: {mcc:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"AUC: {auc:.4f}\")\n    print()\n\n    fold_acc.append(accuracy)\n    fold_loss.append(history.history['val_loss'][-1])  # Using the final validation loss as fold loss\n\n# Calculate average metrics across all folds\navg_accuracy = np.mean(fold_acc)\navg_loss = np.mean(fold_loss)\n\nprint(f\"Average Validation Accuracy: {avg_accuracy:.4f}\")\nprint(f\"Average Validation Loss: {avg_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T21:43:19.238447Z","iopub.execute_input":"2024-07-04T21:43:19.238816Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training fold 1...\nEpoch 1/100\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:51:27.114145Z","iopub.execute_input":"2024-07-04T20:51:27.114497Z","iopub.status.idle":"2024-07-04T20:51:27.124035Z","shell.execute_reply.started":"2024-07-04T20:51:27.114463Z","shell.execute_reply":"2024-07-04T20:51:27.123161Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:51:27.126100Z","iopub.execute_input":"2024-07-04T20:51:27.126588Z","iopub.status.idle":"2024-07-04T20:51:27.132913Z","shell.execute_reply.started":"2024-07-04T20:51:27.126554Z","shell.execute_reply":"2024-07-04T20:51:27.131988Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"rnnamp_model = pd.read_csv('/kaggle/input/ace-dataset/ACE_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:42.859265Z","iopub.execute_input":"2024-07-04T20:12:42.860161Z","iopub.status.idle":"2024-07-04T20:12:42.884958Z","shell.execute_reply.started":"2024-07-04T20:12:42.860118Z","shell.execute_reply":"2024-07-04T20:12:42.883615Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X=rnnamp_model['text']\ny=np.array(rnnamp_model['label'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:12:47.071189Z","iopub.execute_input":"2024-07-04T20:12:47.071622Z","iopub.status.idle":"2024-07-04T20:12:47.083336Z","shell.execute_reply.started":"2024-07-04T20:12:47.071588Z","shell.execute_reply":"2024-07-04T20:12:47.081800Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,models\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import train_test_split\nfrom keras.saving import register_keras_serializable\nfrom tensorflow.keras import regularizers\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:51:33.526178Z","iopub.execute_input":"2024-07-04T20:51:33.526828Z","iopub.status.idle":"2024-07-04T20:51:33.532167Z","shell.execute_reply.started":"2024-07-04T20:51:33.526796Z","shell.execute_reply":"2024-07-04T20:51:33.531202Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:13:42.105244Z","iopub.execute_input":"2024-07-04T20:13:42.106193Z","iopub.status.idle":"2024-07-04T20:13:42.119617Z","shell.execute_reply.started":"2024-07-04T20:13:42.106151Z","shell.execute_reply":"2024-07-04T20:13:42.118137Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"'''\n# Example data\ntexts = X_train\n\n# Tokenize the texts\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts)\n\n# Convert text to sequences of integers\nsequences = tokenizer.texts_to_sequences(texts)\ntest_seq=tokenizer.texts_to_sequences(X_test)\n# Pad sequences to a fixed length\nmaxlen = 100  # Adjust as needed based on your data\npadded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\ntest=pad_sequences(test_seq, maxlen=maxlen, padding='post', truncating='post')\n# Convert to NumPy array\nX_train = tf.constant(padded_sequences)\nX_test = tf.constant(test)\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(X_train)\nprint(X_train.shape)\nX_test=np.array(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:13:45.101060Z","iopub.execute_input":"2024-07-04T20:13:45.102003Z","iopub.status.idle":"2024-07-04T20:13:45.108758Z","shell.execute_reply.started":"2024-07-04T20:13:45.101962Z","shell.execute_reply":"2024-07-04T20:13:45.107303Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(816,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenizer(text, max_len):\n    dic = {'A': 1, 'G': 2, 'V': 3, 'I': 4, 'L': 5, 'F': 6, 'P': 7, 'Y': 8,\n           'M': 9, 'T': 10, 'S': 11, 'H': 12, 'N': 13, 'Q': 14, 'W': 15,\n           'K': 16, 'R': 17, 'D': 18, 'E': 19, 'C': 20}\n    \n    onehot = []\n    t = []\n    \n    for i in range(len(text)):\n        row = []\n        l = []\n        chars = text[i].split()  # split by default splits by any whitespace\n        for j in range(max_len):\n            if j < len(chars):\n                if chars[j] in dic:\n                    row.append(dic[chars[j]])\n                    r = np.zeros(20)\n                    r[dic[chars[j]] - 1] = 1\n                else:\n                    # Handle unknown characters\n                    row.append(0)  # or another default value\n                    r = np.ones(20) * -1\n            else:\n                r = np.ones(20) * -1\n                row.append(0)\n            l.append(r)\n        l = np.array(l)\n        onehot.append(l)\n        t.append(row)\n    \n    onehot = np.array(onehot)\n    t = np.array(t)\n    \n    return t, onehot\nmax_len=30\n\nX_train,onehot_train=tokenizer(X_train,max_len)\nX_test,onehot_test=tokenizer(X_test,max_len)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:43:30.081365Z","iopub.execute_input":"2024-06-28T09:43:30.081795Z","iopub.status.idle":"2024-06-28T09:43:30.347525Z","shell.execute_reply.started":"2024-06-28T09:43:30.081761Z","shell.execute_reply":"2024-06-28T09:43:30.346080Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(\"X_train:\\n\", X_train)\nprint(\"onehot_train:\\n\", onehot_train)\nprint(\"X_test:\\n\", X_test)\nprint(\"onehot_test:\\n\", onehot_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:34:54.035071Z","iopub.execute_input":"2024-06-28T09:34:54.035501Z","iopub.status.idle":"2024-06-28T09:34:54.048326Z","shell.execute_reply.started":"2024-06-28T09:34:54.035469Z","shell.execute_reply":"2024-06-28T09:34:54.047110Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"X_train:\n [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nonehot_train:\n [[[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n ...\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]]\nX_test:\n [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nonehot_test:\n [[[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n ...\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]\n\n [[-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  ...\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]\n  [-1. -1. -1. ... -1. -1. -1.]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"X_train:\\n\", X_train.shape)\nprint(\"onehot_train:\\n\", onehot_train.shape)\nprint(\"X_test:\\n\", X_test.shape)\nprint(\"onehot_test:\\n\", onehot_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T09:35:14.110563Z","iopub.execute_input":"2024-06-28T09:35:14.111013Z","iopub.status.idle":"2024-06-28T09:35:14.117143Z","shell.execute_reply.started":"2024-06-28T09:35:14.110977Z","shell.execute_reply":"2024-06-28T09:35:14.115895Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"X_train:\n (816, 30)\nonehot_train:\n (816, 30, 20)\nX_test:\n (204, 30)\nonehot_test:\n (204, 30, 20)\n","output_type":"stream"}]},{"cell_type":"code","source":"path = \"/kaggle/input/ace-dataset/auto_enco_feat/\"\ndata_ = pd.read_csv(path + 'AEDN-100.csv')\n\ndata_np = np.array(data_)\ndata = data_np[:, 1:]\n# data = data_.iloc[:, :-1].values\n# labels = data_.iloc[:, -1].values\n\nlabel1=np.ones((394,1))#Value can be changed\nlabel2=np.zeros((626,1))\nlabels=np.append(label1,label2)\n\nX_train, X_test, y_train, y_test= train_test_split(data,labels, test_size=0.2, random_state=0)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(816, 100)\n","output_type":"stream"}]},{"cell_type":"code","source":"def positional_encoding(positions, d):\n\n    # initialize a matrix angle_rads of all the angles\n    pos=np.arange(positions)[:, np.newaxis] #Column vector containing the position span [0,1,..., positions]\n    k= np.arange(d)[np.newaxis, :]  #Row vector containing the dimension span [[0, 1, ..., d-1]]\n    i = k//2\n    angle_rads = pos/(10000**(2*i/d)) #Matrix of angles indexed by (pos,i)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    #adds batch axis\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T20:51:48.888258Z","iopub.execute_input":"2024-07-04T20:51:48.888599Z","iopub.status.idle":"2024-07-04T20:51:48.895566Z","shell.execute_reply.started":"2024-07-04T20:51:48.888573Z","shell.execute_reply":"2024-07-04T20:51:48.894662Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"@register_keras_serializable()\nclass TransformerModel(keras.Model):\n    def __init__(self, input_vocab_size, d_model, num_heads, ff_dim, rate=0.1, maxlen=50):\n        super(TransformerModel, self).__init__()\n\n        self.embedding = layers.Embedding(input_vocab_size, d_model)\n        self.PE = positional_encoding(maxlen, d_model)\n        self.transformer_block = TransformerBlock_Encode(d_model, num_heads, ff_dim, rate)\n        self.transformer_block2 = TransformerBlock_decode(d_model, num_heads, ff_dim, rate)\n        self.flatten = layers.Flatten()\n        self.fc1 = layers.Dense(512, activation=\"relu\")\n        self.fc3 = layers.Dense(256, activation=\"relu\")\n        self.fc2 = layers.Dense(1, activation=\"sigmoid\")\n\n    def call(self, inputs, training):\n        x = self.embedding(inputs)\n        #x = x+self.PE\n        y = self.transformer_block(x)\n        x = self.transformer_block2(x,y,y)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc3(x)\n        return self.fc2(x)\n\n# class TransformerBlock_decode(layers.Layer):\n#     def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n#         super(TransformerBlock_decode, self).__init__()\n\n#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)), layers.Dense(d_model)]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n#         self.dropout3 = layers.Dropout(rate)\n\n#     def call(self, inputs, q, k, training=None):\n#         attn_output = self.att(inputs, inputs, inputs)\n#         out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n#         attn_output1 = self.att1(q, k, out1)\n#         out2 = self.layernorm2(out1 + self.dropout2(attn_output1, training=training))\n\n#         ffn_output = self.ffn(out2)\n#         return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n\n\n# class TransformerBlock_Encode(layers.Layer):\n#     def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n#         super(TransformerBlock_Encode, self).__init__()\n#         self.con= layers.Conv1D(256,5,padding='same')\n#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)), layers.Dense(d_model)]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs, training=None):\n#         inputs = self.con(inputs)\n#         attn_output = self.att(inputs, inputs, inputs)\n#         out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n        \n#         ffn_output = self.ffn(out1)\n#         return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n    \nclass TransformerBlock_decode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_decode, self).__init__()\n\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n        self.dropout3 = layers.Dropout(rate)\n\n    def call(self, inputs, q, k, training=None):\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n\n        attn_output1 = self.att1(q, k, out1)\n        out2 = self.layernorm2(out1 + self.dropout2(attn_output1, training=training))\n\n        ffn_output = self.ffn(out2)\n        return self.layernorm3(out2 + self.dropout3(ffn_output, training=training))\n\n\nclass TransformerBlock_Encode(layers.Layer):\n    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock_Encode, self).__init__()\n        self.con= layers.Conv1D(256,5,padding='same')\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training=None):\n        inputs = self.con(inputs)\n        attn_output = self.att(inputs, inputs, inputs)\n        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n        \n        ffn_output = self.ffn(out1)\n        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n\n\n# Define your data loading and preprocessing here\n# Example: X_train, y_train = load_data_and_preprocess()\n\n# Define the model\ninput_vocab_size = 100# Replace with the actual vocabulary size\nd_model = 256\nnum_heads = 4\nff_dim = 128\n\nmodel = TransformerModel(input_vocab_size, d_model, num_heads, ff_dim)\ninitial_learning_rate = 0.0001\n\noptimizer = Adam(learning_rate=initial_learning_rate, clipvalue=0.5)\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"])\n\nfilepath = \"/kaggle/working/\"\n\n# Define a learning rate scheduler\n# def scheduler(epoch, lr):\n#     if epoch < 10:\n#         return lr\n#     else:\n#         return lr * tf.math.exp(-0.1)\n\n# lr_scheduler = LearningRateScheduler(scheduler)\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\n\nsave_dir='aggle/working/'\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n# lr_scheduler = LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\nmodel_checkpoint = ModelCheckpoint(os.path.join(save_dir, f'model_fold_{fold+1}.h5'),\n                                       save_best_only=True,\n                                       monitor='val_loss',\n                                       mode='min')\ncallbacks=[model_checkpoint, early_stopping, reduce_lr]\n# callbacks = [\n#     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n#     tf.keras.callbacks.ModelCheckpoint(\n#         filepath='.weights.h5',  # Save the best model to the specified filepath\n#         monitor='val_accuracy', \n#         save_best_only=True, \n#         mode='auto', \n#         save_weights_only=True  # Set to False to save the entire model\n#     ),reduce_lr\n# ]\n# Train the model\nhistory = model.fit(X_train,y_train,epochs = 100,batch_size=8,validation_split=0.1,callbacks=[callback],verbose=1,shuffle= True)\nmodel.save_weights('model_weights.weights.h5')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-04T21:27:13.693316Z","iopub.execute_input":"2024-07-04T21:27:13.693691Z","iopub.status.idle":"2024-07-04T21:27:13.831476Z","shell.execute_reply.started":"2024-07-04T21:27:13.693659Z","shell.execute_reply":"2024-07-04T21:27:13.830127Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# lr_scheduler = LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfold\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    154\u001b[0m                                        save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m                                        monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    156\u001b[0m                                        mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    157\u001b[0m callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint, early_stopping, reduce_lr]\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# callbacks = [\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m#     tf.keras.callbacks.ModelCheckpoint(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'fold' is not defined"],"ename":"NameError","evalue":"name 'fold' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.load_weights('model_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T21:09:02.733033Z","iopub.execute_input":"2024-07-04T21:09:02.733863Z","iopub.status.idle":"2024-07-04T21:09:06.011558Z","shell.execute_reply.started":"2024-07-04T21:09:02.733830Z","shell.execute_reply":"2024-07-04T21:09:06.010464Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 104 variables whereas the saved optimizer has 84 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/best_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:295\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    293\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n","\u001b[0;31mValueError\u001b[0m: A total of 7 objects could not be loaded. Example error message for object <Dense name=dense_62, built=True>:\n\nLayer 'dense_62' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=dense_62, built=True>, <Dense name=dense_61, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=layer_normalization_40, built=True>]"],"ename":"ValueError","evalue":"A total of 7 objects could not be loaded. Example error message for object <Dense name=dense_62, built=True>:\n\nLayer 'dense_62' expected 2 variables, but received 0 variables during loading. Expected: ['kernel', 'bias']\n\nList of objects that could not be loaded:\n[<Dense name=dense_62, built=True>, <Dense name=dense_61, built=True>, <EinsumDense name=key, built=True>, <EinsumDense name=attention_output, built=True>, <EinsumDense name=query, built=True>, <EinsumDense name=value, built=True>, <LayerNormalization name=layer_normalization_40, built=True>]","output_type":"error"}]},{"cell_type":"code","source":"model1=model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection, metrics\ny_pred=model.predict(X_train)\nfpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred, pos_label=1)\nauc=metrics.auc(fpr, tpr)\n\ny_pred[y_pred>0.5]=1\ny_pred[y_pred<0.5]=0\n\ncv_preds = y_pred\nprint('combined train datasets')\nname='deep learning'\nprint(\"%s: Accuracy %0.2f%%\" % (name, 100*metrics.accuracy_score(y_train, cv_preds)))\nprint(\"%s: Precision-Recall %0.2f%%\" % (name, 100*metrics.average_precision_score(y_train, cv_preds)))\nprint(\"%s: Matthews Coefficient %0.2f%%\" % (name, 100*metrics.matthews_corrcoef(y_train, cv_preds)))\nprint(\"%s: Cohen Kappa Score %0.2f%%\" % (name, 100*metrics.cohen_kappa_score(y_train, cv_preds)))\nprint(\"%s: F1-Score %0.2f%%\" % (name, 100*metrics.f1_score(y_train, cv_preds)))\nprint(\"%s: AUC Score %0.2f%%\" % (name, 100*auc))\ntarget_names = ['low 0', 'high 1']\nprint(metrics.classification_report(y_train, cv_preds, target_names=target_names))\n\n# Predictions Validation Set\nprint('combined test datasets')\ny_pred2=model.predict(X_test)\nl=np.zeros(len(y_pred2))\nl=l.reshape(-1,1)\nl[y_pred2>=0.5]=1\nl[y_pred2<0.5]=0\ncv_preds2 = l\nprint(\"%s: Accuracy %0.2f%%\" % (name, 100*metrics.accuracy_score(y_test, cv_preds2)))\nprint(\"%s: Precision %0.2f%%\" % (name, 100*metrics.precision_score(y_test, cv_preds2)))\nprint(\"%s: Recall %0.2f%%\" % (name, 100*metrics.recall_score(y_test, cv_preds2)))\nprint(\"%s: Matthews Coefficient %0.2f%%\" % (name, 100*metrics.matthews_corrcoef(y_test, cv_preds2)))\nprint(\"%s: Cohen Kappa Score %0.2f%%\" % (name, 100*metrics.cohen_kappa_score(y_test, cv_preds2)))\nprint(\"%s: F1-Score %0.2f%%\" % (name, 100*metrics.f1_score(y_test, cv_preds2)))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred2, pos_label=1)\nauc=metrics.auc(fpr, tpr)\nprint(\"%s: AUC Score %0.2f%%\" % (name, 100*auc))\n\ntarget_names = ['low 0', 'high 1']\nprint(metrics.classification_report(y_test, cv_preds2, target_names=target_names))","metadata":{},"execution_count":null,"outputs":[]}]}