{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNG6R6K8iNKuNZDH1xv4fNZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c7PdaVKjkqS1"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","import transformers\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Load protein sequences from a FASTA file\n","fasta_file = \"/content/mus_train.fasta\"\n","sequences = []\n","labels = []\n","\n","with open(fasta_file, \"r\") as f:\n","    record_lines = []\n","    for line in f:\n","        if line.startswith(\">\"):\n","            if record_lines:\n","                sequences.append(\"\".join(record_lines))\n","                record_lines = []\n","            labels.append(1 if \"positive\" in line else 0)  # Adjust labels accordingly\n","        else:\n","            record_lines.append(line.strip())\n","    if record_lines:\n","        sequences.append(\"\".join(record_lines))\n","\n","# Convert sequences and labels to tensors\n","data = {'sequence': sequences, 'label': labels}\n","df = pd.DataFrame(data)\n","'''\n","# Sample protein sequences in FASTA format\n","fasta_data = \"\"\"\n",">positive\n","MVKVGVNGFGRIGRLVTRAAFNSGKVDVAVLDSGGTFIACSDKSAAVEAPQF\n",">positive\n","KFYNLCPMMEKEGKIALNDQKIVQLLREKTGEEKILVNLAARYFSYETSSA\n",">positive\n","MGDVEKGKKIFIMKCSQCHTVEKGGKHKTGPNEKGKKIFVQKCSQCHTVETG\n",">positive\n","MKTVRQERLKSIVRILERSKEPVSGAELRGKILRLQHPNIVRLGESNVDLV\n",">positive\n","MGIDYKAWRGELTQIKYTANPMFMLGPNIEGTTGEIVGTGGVNGEVTAWKG\n",">seq6\n","MADVLTLEDSWLSFYHLLFDGSDGKKQYAEELLDTLLRTEVAGYLTADLKS\n",">seq7\n","MQSNGFYLRAKAISTTAVGGVVEQSGVDVANLVGSAKILNTELLDTLVQQ\n",">seq8\n","MHGVPAKTFLFSTQPATAKKGVGGNNTFVRVVPQNKILSKHFSNAAAKKQ\n",">seq9\n","MATSLVNFSGELDTSAPVVRKPRKGKLMSSLGLPFSDVPVGPMGVGVSAI\n",">seq10\n","MKMTGMTLAGGFGYAAKANIVGNTPTSLRDAVKLADAVAAKGIKRVTYSK\"\"\"\n","\n","# Convert FASTA data to a DataFrame\n","sequences = []\n","labels = []\n","\n","for record in fasta_data.strip().split(\"\\n\"):\n","    if record.startswith(\">\"):\n","        labels.append(1 if \"positive\" in record else 0)  # Adjust labels accordingly\n","    else:\n","        sequences.append(record)\n","\n","data = {'sequence': sequences, 'label': labels}\n","df = pd.DataFrame(data)\n","'''\n","# Initialize BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Convert sequences and labels to tensors\n","sequences = df['sequence'].tolist()\n","labels = df['label'].tolist()\n","\n","\n","# Tokenize sequences and prepare data for BERT\n","encodings = tokenizer(sequences, truncation=True, padding=True, max_length=128)\n","input_ids = torch.tensor(encodings['input_ids'])\n","attention_mask = torch.tensor(encodings['attention_mask'])\n","labels = torch.tensor(labels)\n","print(labels)\n","# Define cross-validation\n","num_folds = 5\n","skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n","\n","# Initialize lists to store accuracy scores\n","accuracy_scores = []\n","test_label =[]\n","yscore= []\n","for fold, (train_idx, val_idx) in enumerate(skf.split(input_ids, labels)):\n","    print(f\"Fold {fold+1}/{num_folds}\")\n","\n","    # Split data into training and validation sets\n","    train_input_ids = input_ids[train_idx]\n","    train_attention_mask = attention_mask[train_idx]\n","    train_labels = labels[train_idx]\n","\n","    val_input_ids = input_ids[val_idx]\n","    val_attention_mask = attention_mask[val_idx]\n","    val_labels = labels[val_idx]\n","\n","    train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n","    val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n","\n","    # Define optimizer and training loop\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","    for epoch in range(3):  # adjust the number of epochs as needed\n","        model.train()\n","        for batch in train_loader:\n","            optimizer.zero_grad()\n","            input_batch = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","            outputs = model(**input_batch)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","\n","    # Evaluate on validation data\n","    model.eval()\n","    val_loader = DataLoader(val_dataset, batch_size=32)\n","    predictions = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_batch = {'input_ids': batch[0], 'attention_mask': batch[1]}\n","            outputs = model(**input_batch)\n","            logits = outputs.logits\n","            test_label.append(logits)\n","            preds = torch.argmax(logits, dim=1)\n","            predictions.extend(preds.cpu().numpy())\n","\n","    accuracy = accuracy_score(val_labels, predictions)\n","    accuracy_scores.append(accuracy)\n","    print(f\"Validation Accuracy: {accuracy:.4f}\")\n","    print(test_label)\n","# Calculate and print average accuracy over folds\n","average_accuracy = np.mean(accuracy_scores)\n","print(f\"Average Accuracy: {average_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khHgxg_-ez4X","outputId":"e0f0e54e-d622-4b08-e30c-64bfdedfa27d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([0, 0, 0,  ..., 0, 0, 0])\n","Fold 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]}]}