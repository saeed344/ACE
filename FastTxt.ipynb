{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15qN9TbdbqmpMtLn65XIC07ki1YFl68lp","authorship_tag":"ABX9TyPoNN0i9hA3yi+lQG5sBvfH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emqQEFteGoes","executionInfo":{"status":"ok","timestamp":1684335227903,"user_tz":-300,"elapsed":1641718,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"e4e989a0-0208-4b4a-8ab8-a9cce0854eee"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvnEU_59N5rC","executionInfo":{"status":"ok","timestamp":1684335500945,"user_tz":-300,"elapsed":19385,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"d4735cdd-041f-4b05-e38a-2d9e8a7e84cb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4BGL11V2DRIq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684335520096,"user_tz":-300,"elapsed":19153,"user":{"displayName":"Dr. Saeed Ahmed","userId":"15898076895818453159"}},"outputId":"f9a19965-c3d4-4d6b-c167-dfaf670e58ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[[-0.13529807  0.06433191  0.03500054 ...  0.01327619 -0.11958317\n","   0.10745705]\n"," [-0.13797145  0.06834678  0.03556608 ...  0.0139749  -0.13021139\n","   0.11581534]\n"," [-0.0670877   0.048177    0.00020939 ...  0.000347    0.06653122\n","  -0.09902307]\n"," ...\n"," [-0.08067543  0.05662995  0.00589048 ...  0.00258245  0.03493547\n","  -0.06475744]\n"," [-0.10267424  0.05997989  0.01753549 ...  0.00598394 -0.01219003\n","  -0.00587908]\n"," [-0.09141966  0.06659426  0.00959213 ...  0.00379118  0.01915838\n","  -0.04591381]]\n"]}],"source":["import numpy as np\n","from gensim.models import FastText\n","import numpy as np\n","import torch\n","from transformers import BertModel, BertTokenizer\n","\n","import pandas as pd\n","import numpy as np\n","import re, os, sys\n","from itertools import product\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","def read_nucleotide_sequences(file):\n","    if os.path.exists(file) == False:\n","        print('Error: file %s does not exist.' % file)\n","        sys.exit(1)\n","    with open(file) as f:\n","        records = f.read()\n","    if re.search('>', records) == None:\n","        print('Error: the input file %s seems not in FASTA format!' % file)\n","        sys.exit(1)\n","    records = records.split('>')[1:]\n","    fasta_sequences = []\n","    for fasta in records:\n","        array = fasta.split('\\n')\n","        header, sequence = array[0].split()[0], re.sub('[^ACGTU-]', '-', ''.join(array[1:]).upper())\n","        header_array = header.split('|')\n","        name = header_array[0]\n","        label = header_array[1] if len(header_array) >= 2 else '0'\n","        label_train = header_array[2] if len(header_array) >= 3 else 'training'\n","        sequence = re.sub('U', 'T', sequence)\n","        fasta_sequences.append(sequence)\n","    return fasta_sequences\n","\n","#!/usr/bin/env python\n","#_*_coding:utf-8_*_\n","\n","import re\n","\n","def check_fasta_with_equal_length(fastas):\n","    status = True\n","    lenList = set()\n","    for i in fastas:\n","        lenList.add(len(i[1]))\n","    if len(lenList) == 1:\n","        return True\n","    else:\n","        return False\n","\n","def get_min_sequence_length(fastas):\n","    minLen = 10000\n","    for i in fastas:\n","        if minLen > len(i[1]):\n","            minLen = len(i[1])\n","    return minLen\n","\n","def get_min_sequence_length_1(fastas):\n","    minLen = 10000\n","    for i in fastas:\n","        if minLen > len(re.sub('-', '', i[1])):\n","            minLen = len(re.sub('-', '', i[1]))\n","    return minLen\n","def readFasta(file):\n","    if os.path.exists(file) == False:\n","        print('Error: \"' + file + '\" does not exist.')\n","        sys.exit(1)\n","\n","    with open(file) as f:\n","        records = f.read()\n","\n","    if re.search('>', records) == None:\n","        print('The input file seems not in fasta format.')\n","        sys.exit(1)\n","\n","    records = records.split('>')[1:]\n","    myFasta = []\n","    for fasta in records:\n","        array = fasta.split('\\n')\n","        name, sequence = array[0].split()[0], re.sub('[^ARNDCQEGHILKMFPSTWYV-]', '-', ''.join(array[1:]).upper())\n","        myFasta.append([name, sequence])\n","    return myFasta\n","\n","\n","def extract_features(dna_sequences, vector_size=100, window=5, min_count=1):\n","    # Prepare data for FastText\n","    tokenized_sequences = [list(sequence) for sequence in dna_sequences]\n","\n","    # Train FastText model\n","    model = FastText(tokenized_sequences, vector_size=vector_size, window=window, min_count=min_count)\n","\n","    # Extract features\n","    features = np.zeros((len(dna_sequences), vector_size))\n","    for i, sequence in enumerate(tokenized_sequences):\n","        for token in sequence:\n","            features[i] += model.wv[token]\n","        features[i] /= len(sequence)\n","\n","    return features\n","\n","# Example usage\n","dna_sequences = ['ATCGATCGATCG', 'CGATCGATCGATCG']\n","\n","fastas = read_nucleotide_sequences('/content/drive/MyDrive/Research Work/Arabidopsis_train.fasta')\n","features = extract_features(fastas)\n","data_csv=pd.DataFrame(features)\n","data_csv.to_csv('/content/drive/MyDrive/Research Work/FastText_Arabidopsis_train.csv')\n","\n","print(features)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"0Kcfd-aBGnT2"},"execution_count":null,"outputs":[]}]}